# analytics/autofill_writer_samples.py
#
# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–ø–æ–ª–Ω—è–µ—Ç writer_samples –∏–∑ —Ö–æ—Ä–æ—à–∏—Ö –ø–æ—Å—Ç–æ–≤:
# 1) –ë–µ—Ä—ë—Ç –ø–æ—Å—Ç—ã —Å –≤—ã—Å–æ–∫–∏–º quality_score –∏–∑ post_quality
# 2) –ü—Ä–æ—Å–∏—Ç –ª–æ–∫–∞–ª—å–Ω—É—é Qwen –≤—ã–¥–µ–ª–∏—Ç—å goal / topic_brief / final_post
# 3) –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ writer_samples
#
# –ó–∞–ø—É—Å–∫:
#   python -m analytics.autofill_writer_samples

from __future__ import annotations
import os
import sys
import json
import re
import asyncio
from datetime import datetime
from typing import Optional, Dict, Any, List

import asyncpg
import torch
from dotenv import load_dotenv
import pathlib

# === –ë–∞–∑–æ–≤–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–æ–µ–∫—Ç–∞ ===
BASE_DIR = pathlib.Path(__file__).resolve().parents[1]
if str(BASE_DIR) not in sys.path:
    sys.path.insert(0, str(BASE_DIR))

load_dotenv(BASE_DIR / ".env")

# –õ–æ–∫–∞–ª—å–Ω—ã–π –∑–∞–≥—Ä—É–∑—á–∏–∫ –º–æ–¥–µ–ª–∏ (–∫–∞–∫ –≤ judge_quality_llm)
from Models.qwen_loader import load_tokenizer_model

DB = {
    "host": os.getenv("POSTGRES_HOST", "127.0.0.1"),
    "port": int(os.getenv("POSTGRES_PORT", 5432)),
    "database": os.getenv("POSTGRES_DB", "engagex"),
    "user": os.getenv("POSTGRES_USER", "engagex"),
    "password": os.getenv("POSTGRES_PASSWORD", "engagex"),
}

# –ü–æ—Ä–æ–≥ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ—Å—Ç–∞, –≤—ã—à–µ –∫–æ—Ç–æ—Ä–æ–≥–æ —Å—á–∏—Ç–∞–µ–º –µ–≥–æ –≥–æ–¥–Ω—ã–º –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞
MIN_QUALITY_SCORE = float(os.getenv("WRITER_MIN_SCORE", "70"))

# –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å—Ç–æ–≤ –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ–≥–æ–Ω
MAX_POSTS = int(os.getenv("WRITER_MAX_POSTS", "100"))

# === –ü—Ä–æ–º–ø—Ç –ø–æ–¥ —Ä–∞–∑–º–µ—Ç–∫—É ===

SYSTEM_MSG = (
    "–¢—ã ‚Äî –º–µ—Ç–æ–¥–∏—Å—Ç –∏ —Ä–µ–¥–∞–∫—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π –≥–æ—Ç–æ–≤–∏—Ç –æ–±—É—á–∞—é—â–∏–µ –ø–∞—Ä—ã –¥–ª—è –º–æ–¥–µ–ª–∏-–ø–∏—Å–∞—Ç–µ–ª—è.\n"
    "–ù–∞ –≤—Ö–æ–¥ —Ç–µ–±–µ –¥–∞—ë—Ç—Å—è —Ä–µ–∞–ª—å–Ω—ã–π –ø–æ—Å—Ç –∏–∑ Telegram-–∫–∞–Ω–∞–ª–∞ –ø—Ä–æ TON / –∫—Ä–∏–ø—Ç—É.\n\n"
    "–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –∞–∫–∫—É—Ä–∞—Ç–Ω–æ —Ä–∞–∑–æ–±—Ä–∞—Ç—å —ç—Ç–æ—Ç –ø–æ—Å—Ç –∏ –≤–µ—Ä–Ω—É—Ç—å –û–î–ò–ù JSON:\n"
    "{\n"
    "  \"goal\": <—Å—Ç—Ä–æ–∫–∞>,\n"
    "  \"topic_brief\": <—Å—Ç—Ä–æ–∫–∞>,\n"
    "  \"final_post\": <—Å—Ç—Ä–æ–∫–∞>\n"
    "}\n\n"
    "–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:\n"
    "1) goal ‚Äî –æ–¥–Ω–∞ –∫–æ—Ä–æ—Ç–∫–∞—è —Ñ—Ä–∞–∑–∞, –æ–ø–∏—Å—ã–≤–∞—é—â–∞—è —Ü–µ–ª—å –ø–æ—Å—Ç–∞ (—á—Ç–æ –æ–Ω –¥–æ–ª–∂–µ–Ω —Å–¥–µ–ª–∞—Ç—å –¥–ª—è —á–∏—Ç–∞—Ç–µ–ª—è). "
    "–ë–ï–ó —Å–ª–æ–≤ \"–¶–µ–ª—å –ø–æ—Å—Ç–∞\" –∏ –±–µ–∑ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.\n"
    "2) topic_brief ‚Äî 3‚Äì7 –ª–∞–∫–æ–Ω–∏—á–Ω—ã—Ö –ø—É–Ω–∫—Ç–æ–≤, –∫–∞–∂–¥—ã–π —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏ –∏ —Å –Ω–∞—á–∞–ª–æ–º —á–µ—Ä–µ–∑ –¥–µ—Ñ–∏—Å –∏–ª–∏ —Ç–∏—Ä–µ.\n"
    "   –ó–¥–µ—Å—å –∫—Ä–∞—Ç–∫–æ —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç—Å—è —Ç–µ–º–∞: —á—Ç–æ –∑–∞ —Å–æ–±—ã—Ç–∏–µ / –ø—Ä–æ–¥—É–∫—Ç / –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ, –∫–∞–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ –∞—Å–ø–µ–∫—Ç—ã.\n"
    "   –ù–ï –∫–æ–ø–∏—Ä—É–π –¥–æ—Å–ª–æ–≤–Ω–æ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –ø–æ—Å—Ç–∞ –∏ –Ω–µ –ø–∏—à–∏ —Å—é–¥–∞ —Å–∞–º –ø–æ—Å—Ç.\n"
    "3) final_post ‚Äî –æ—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –ø–æ—Å—Ç–∞ –¥–ª—è Telegram-–∫–∞–Ω–∞–ª–∞.\n"
    "   –ü–∏—à–∏ –≤ –∂–∏–≤–æ–º, –¥–µ–ª–æ–≤–æ–º —Å—Ç–∏–ª–µ, –±–µ–∑ –≤–æ–¥—ã –∏ –∫–ª–∏–∫–±–µ–π—Ç–∞.\n"
    "   –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π —Å–ª–æ–≤–∞ \"–¶–µ–ª—å\", \"–ö—Ä–∞—Ç–∫–æ\" –∏ —Ç.–ø. ‚Äî —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π –ø–æ—Å—Ç.\n"
    "   –ú–æ–∂–Ω–æ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞—Ç—å –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞—Ç—å –∏—Å—Ö–æ–¥–Ω–∏–∫, –Ω–æ —Å–º—ã—Å–ª –¥–æ–ª–∂–µ–Ω —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å—Å—è.\n\n"
    "–í–ï–†–ù–ò –¢–û–õ–¨–ö–û –û–î–ò–ù –í–ê–õ–ò–î–ù–´–ô JSON. –ù–ò–ß–ï–ì–û –ë–û–õ–¨–®–ï."
)

USER_TEMPLATE = (
    "–ö–∞–Ω–∞–ª: {channel}\n\n"
    "–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø–æ—Å—Ç:\n\"\"\"\n{post}\n\"\"\"\n\n"
    "–°—Ñ–æ—Ä–º–∏—Ä—É–π JSON —Å –ø–æ–ª—è–º–∏ goal, topic_brief, final_post –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–µ–π –≤—ã—à–µ."
)

# === –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –º–æ–¥–µ–ª—å—é ===

_tokenizer = None
_model = None


def ensure_model():
    """
    –õ–µ–Ω–∏–≤–æ –∑–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä –∏ –º–æ–¥–µ–ª—å –æ–¥–∏–Ω —Ä–∞–∑ –Ω–∞ –ø—Ä–æ—Ü–µ—Å—Å.
    """
    global _tokenizer, _model
    if _tokenizer is not None and _model is not None:
        return

    print(f"[{datetime.now().isoformat()}] –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞–∑–º–µ—Ç–∫–∏ writer_samples...")
    _tokenizer, _model = load_tokenizer_model()

    try:
        device = _model.device
    except Exception:
        params = list(_model.parameters())
        device = params[0].device if params else torch.device("cpu")

    print(f"[{datetime.now().isoformat()}] –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ {device}")


def extract_json(text: str) -> Optional[Dict[str, Any]]:
    """
    –ë–æ–ª–µ–µ —É—Å—Ç–æ–π—á–∏–≤—ã–π JSON-—ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä:
    - —á–∏—Å—Ç–∏–º –∫–æ–¥-–±–ª–æ–∫–∏ ```...```
    - —É–±–∏—Ä–∞–µ–º —É–ø—Ä–∞–≤–ª—è—é—â–∏–µ —Å–∏–º–≤–æ–ª—ã
    - —Å–∫–∞–Ω–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫—É –ø–æ –≤—Å–µ–º '{'
    - –Ω–∞ –∫–∞–∂–¥–æ–º –ø–æ–ª–æ–∂–µ–Ω–∏–∏ –ø—ã—Ç–∞–µ–º—Å—è —Å–¥–µ–ª–∞—Ç—å raw_decode
    - –∫–∞–∫ —Ç–æ–ª—å–∫–æ —É–¥–∞–ª–æ—Å—å ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ–±—ä–µ–∫—Ç
    –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –≤—ã—à–ª–æ ‚Äî None.
    """
    if not text:
        return None

    # —É–±–∏—Ä–∞–µ–º –∫–æ–¥-–±–ª–æ–∫–∏, —á—Ç–æ–±—ã –Ω–µ –ª–æ–º–∞—Ç—å –ø–∞—Ä—Å–µ—Ä
    text = re.sub(r"```.*?```", " ", text, flags=re.S)
    # —É–±–∏—Ä–∞–µ–º —É–ø—Ä–∞–≤–ª—è—é—â–∏–µ —Å–∏–º–≤–æ–ª—ã
    text = re.sub(r"[\x00-\x08\x0b-\x0c\x0e-\x1f\x7f-\x9f]", "", text)
    text = text.strip()

    decoder = json.JSONDecoder()

    for m in re.finditer(r"\{", text):
        start = m.start()
        try:
            obj, _ = decoder.raw_decode(text[start:])
            if isinstance(obj, dict):
                return obj
        except Exception:
            continue

    return None


def build_messages(channel: str, post_text: str) -> List[Dict[str, str]]:
    return [
        {"role": "system", "content": SYSTEM_MSG},
        {
            "role": "user",
            "content": USER_TEMPLATE.format(channel=channel, post=post_text[:16000]),
        },
    ]


def run_inference(channel: str, post_text: str) -> Optional[Dict[str, Any]]:
    """
    –ü—Ä–æ–≥–æ–Ω –æ–¥–Ω–æ–≥–æ –ø–æ—Å—Ç–∞ —á–µ—Ä–µ–∑ Qwen, –ø–æ–ø—ã—Ç–∫–∞ –≤—ã—Ç–∞—â–∏—Ç—å JSON.
    """
    ensure_model()

    messages = build_messages(channel, post_text)

    # Qwen chat template
    try:
        inputs = _tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_tensors="pt",
        )
    except TypeError:
        inputs = _tokenizer.apply_chat_template(
            messages,
            return_tensors="pt",
        )

    if isinstance(inputs, torch.Tensor):
        input_ids = inputs
        attention_mask = None
    elif isinstance(inputs, dict):
        input_ids = inputs.get("input_ids")
        attention_mask = inputs.get("attention_mask")
    else:
        input_ids = inputs
        attention_mask = None

    try:
        device = _model.device
    except Exception:
        params = list(_model.parameters())
        device = params[0].device if params else torch.device("cpu")

    input_ids = input_ids.to(device)
    if attention_mask is not None:
        attention_mask = attention_mask.to(device)

    gen_kwargs = dict(
        input_ids=input_ids,
        max_new_tokens=512,
        do_sample=False,
        pad_token_id=getattr(_tokenizer, "eos_token_id", None),
        eos_token_id=getattr(_tokenizer, "eos_token_id", None),
    )
    if attention_mask is not None:
        gen_kwargs["attention_mask"] = attention_mask

    with torch.inference_mode():
        out = _model.generate(**gen_kwargs)

    gen_ids = out[0][input_ids.shape[-1]:]
    gen_text = _tokenizer.decode(gen_ids, skip_special_tokens=True)

    js = extract_json(gen_text)
    return js


# === –†–∞–±–æ—Ç–∞ —Å –ë–î ===

CREATE_WRITER_SAMPLES_SQL = """
CREATE TABLE IF NOT EXISTS writer_samples (
    id SERIAL PRIMARY KEY,
    sample_type VARCHAR(50) NOT NULL DEFAULT 'post',
    source_post_id INTEGER NOT NULL REFERENCES posts(id) ON DELETE CASCADE,
    channel_username VARCHAR(255) NOT NULL,
    goal TEXT NOT NULL,
    topic_brief TEXT NOT NULL,
    final_post TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE (sample_type, source_post_id)
);
"""

SELECT_CANDIDATES_SQL = """
SELECT
    p.id AS post_id,
    p.channel_username,
    COALESCE(cp.clean_text, p.post_text) AS text,
    pq.quality_score,
    p.ingest_status
FROM posts p
JOIN post_quality pq
    ON pq.post_id = p.id
LEFT JOIN clean_posts cp
    ON cp.source_post_id = p.id
WHERE
    pq.is_good = true
    AND pq.quality_score >= $1
    AND p.ingest_status = 'done'
    AND COALESCE(cp.clean_text, p.post_text) IS NOT NULL
    AND TRIM(COALESCE(cp.clean_text, p.post_text)) <> ''
    AND NOT EXISTS (
        SELECT 1 FROM writer_samples ws WHERE ws.source_post_id = p.id
            AND ws.sample_type = 'post'
    )
ORDER BY pq.quality_score DESC, p.id
LIMIT $2;
"""

INSERT_WRITER_SAMPLE_SQL = """
INSERT INTO writer_samples (
    sample_type,
    source_post_id,
    channel_username,
    goal,
    topic_brief,
    final_post
) VALUES ($1, $2, $3, $4, $5, $6);
"""


async def ensure_writer_samples_table(conn: asyncpg.Connection) -> None:
    """
    –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º, —á—Ç–æ writer_samples —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.
    –≠—Ç–æ –¥–∞—ë—Ç –∞–≤—Ç–æ–Ω–æ–º–Ω–æ—Å—Ç—å: –º–æ–∂–Ω–æ –∑–∞–ø—É—Å–∫–∞—Ç—å —Å–∫—Ä–∏–ø—Ç –¥–∞–∂–µ
    –µ—Å–ª–∏ –ø–∞—Ä—Å–µ—Ä –µ—â—ë –Ω–µ —Å–æ–∑–¥–∞—ë—Ç —ç—Ç—É —Ç–∞–±–ª–∏—Ü—É.
    """
    await conn.execute(CREATE_WRITER_SAMPLES_SQL)


async def fetch_candidates(conn) -> List[asyncpg.Record]:
    rows = await conn.fetch(SELECT_CANDIDATES_SQL, MIN_QUALITY_SCORE, MAX_POSTS)
    print(f"[{datetime.now().isoformat()}] –ù–∞–π–¥–µ–Ω–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è —Ä–∞–∑–º–µ—Ç–∫–∏: {len(rows)}")
    return rows


async def save_writer_sample(
    conn,
    post_id: int,
    channel: str,
    goal: str,
    topic_brief: str,
    final_post: str,
):
    await conn.execute(
        INSERT_WRITER_SAMPLE_SQL,
        "post",              # sample_type ‚Äî —è–≤–Ω–æ –ø–æ–º–µ—á–∞–µ–º, —á—Ç–æ —ç—Ç–æ –ø–æ—Å—Ç
        post_id,
        channel,
        goal.strip(),
        topic_brief.strip(),
        final_post.strip(),
    )


# === –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª ===

async def main():
    print(f"[{datetime.now().isoformat()}] üöÄ –ê–≤—Ç–æ—Ä–∞–∑–º–µ—Ç–∫–∞ writer_samples —Å—Ç–∞—Ä—Ç—É–µ—Ç...")
    conn = await asyncpg.connect(**DB)
    try:
        # –ù–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π —Å–æ–∑–¥–∞—ë–º writer_samples, –µ—Å–ª–∏ –µ—ë –µ—â—ë –Ω–µ—Ç
        await ensure_writer_samples_table(conn)

        rows = await fetch_candidates(conn)
        if not rows:
            print(f"[{datetime.now().isoformat()}] –ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –ø–æ—Å—Ç–æ–≤ ‚Äî –≤—ã—Ö–æ–¥–∏–º.")
            return

        processed = 0
        skipped = 0

        for r in rows:
            post_id = r["post_id"]
            channel = r["channel_username"]
            text = (r["text"] or "").strip()
            ingest_status = r["ingest_status"]

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π safety-check, –µ—Å–ª–∏ –≤–¥—Ä—É–≥ SQL –ø–æ–º–µ–Ω—è—é—Ç
            if ingest_status != "done":
                print(
                    f"[{datetime.now().isoformat()}] ‚ö†Ô∏è post_id={post_id} —Å ingest_status={ingest_status}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º."
                )
                skipped += 1
                continue

            if not text:
                skipped += 1
                continue

            print(f"[{datetime.now().isoformat()}] ‚Üí –û–±—Ä–∞–±–æ—Ç–∫–∞ post_id={post_id} ({channel}), ingest_status={ingest_status}")

            js = run_inference(channel, text)
            if not js:
                print(f"[{datetime.now().isoformat()}] ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã—Ç–∞—â–∏—Ç—å JSON –¥–ª—è post_id={post_id}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º.")
                skipped += 1
                continue

            goal = str(js.get("goal", "") or "").strip()
            topic_brief = str(js.get("topic_brief", "") or "").strip()
            final_post = str(js.get("final_post", "") or "").strip()

            if not goal or not topic_brief or not final_post:
                print(f"[{datetime.now().isoformat()}] ‚ö†Ô∏è –ü—É—Å—Ç—ã–µ –ø–æ–ª—è –≤ JSON –¥–ª—è post_id={post_id}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º.")
                skipped += 1
                continue

            await save_writer_sample(conn, post_id, channel, goal, topic_brief, final_post)
            processed += 1
            print(f"[{datetime.now().isoformat()}] ‚úÖ post_id={post_id} ‚Üí –∑–∞–ø–∏—Å–∞–Ω –≤ writer_samples")

        print(f"[{datetime.now().isoformat()}] –ì–æ—Ç–æ–≤–æ. –£—Å–ø–µ—à–Ω–æ: {processed}, –ø—Ä–æ–ø—É—â–µ–Ω–æ: {skipped}")

    finally:
        await conn.close()
        print(f"[{datetime.now().isoformat()}] üîå –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –ë–î –∑–∞–∫—Ä—ã—Ç–æ.")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("–û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.")
