# analytics/autofill_writer_samples.py
#
# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–ø–æ–ª–Ω—è–µ—Ç writer_samples –∏–∑ —Ö–æ—Ä–æ—à–∏—Ö –ø–æ—Å—Ç–æ–≤:
# 1) –ë–µ—Ä—ë—Ç –ø–æ—Å—Ç—ã —Å –≤—ã—Å–æ–∫–∏–º quality_score –∏–∑ post_quality
# 2) –ü—Ä–æ—Å–∏—Ç –ª–æ–∫–∞–ª—å–Ω—É—é Qwen –≤—ã–¥–µ–ª–∏—Ç—å goal / topic_brief / final_post
# 3) –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ writer_samples —Å gen_status = 'ok' –∏–ª–∏ 'error'
#
# –ó–∞–ø—É—Å–∫:
#   python -m analytics.autofill_writer_samples
#   –∏–ª–∏
#   python analytics/autofill_writer_samples.py

from __future__ import annotations
import os
import sys
import json
import re
import asyncio
from datetime import datetime
from typing import Optional, Dict, Any, List

import asyncpg
import torch
from dotenv import load_dotenv
import pathlib

# === –ë–∞–∑–æ–≤–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–æ–µ–∫—Ç–∞ ===
BASE_DIR = pathlib.Path(__file__).resolve().parents[1]
if str(BASE_DIR) not in sys.path:
    sys.path.insert(0, str(BASE_DIR))

load_dotenv(BASE_DIR / ".env")

# –õ–æ–∫–∞–ª—å–Ω—ã–π –∑–∞–≥—Ä—É–∑—á–∏–∫ –º–æ–¥–µ–ª–∏ (–∫–∞–∫ –≤ judge_quality_llm)
from Models.qwen_loader import load_tokenizer_model

DB = {
    "host": os.getenv("POSTGRES_HOST", "127.0.0.1"),
    "port": int(os.getenv("POSTGRES_PORT", 5432)),
    "database": os.getenv("POSTGRES_DB", "engagex"),
    "user": os.getenv("POSTGRES_USER", "engagex"),
    "password": os.getenv("POSTGRES_PASSWORD", "engagex"),
}

# –ü–æ—Ä–æ–≥ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ—Å—Ç–∞, –≤—ã—à–µ –∫–æ—Ç–æ—Ä–æ–≥–æ —Å—á–∏—Ç–∞–µ–º –µ–≥–æ –≥–æ–¥–Ω—ã–º –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞
MIN_QUALITY_SCORE = float(os.getenv("WRITER_MIN_SCORE", "70"))

# –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å—Ç–æ–≤ –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ–≥–æ–Ω
MAX_POSTS = int(os.getenv("WRITER_MAX_POSTS", "10000000"))

# === –ü—Ä–æ–º–ø—Ç –ø–æ–¥ —Ä–∞–∑–º–µ—Ç–∫—É ===

SYSTEM_MSG = (
    "–¢—ã ‚Äî –º–µ—Ç–æ–¥–∏—Å—Ç –∏ —Ä–µ–¥–∞–∫—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π –≥–æ—Ç–æ–≤–∏—Ç –æ–±—É—á–∞—é—â–∏–µ –ø–∞—Ä—ã –¥–ª—è –º–æ–¥–µ–ª–∏-–ø–∏—Å–∞—Ç–µ–ª—è.\n"
    "–ù–∞ –≤—Ö–æ–¥ —Ç–µ–±–µ –¥–∞—ë—Ç—Å—è —Ä–µ–∞–ª—å–Ω—ã–π –ø–æ—Å—Ç –∏–∑ Telegram-–∫–∞–Ω–∞–ª–∞ –ø—Ä–æ TON / –∫—Ä–∏–ø—Ç—É.\n\n"
    "–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –∞–∫–∫—É—Ä–∞—Ç–Ω–æ —Ä–∞–∑–æ–±—Ä–∞—Ç—å —ç—Ç–æ—Ç –ø–æ—Å—Ç –∏ –≤–µ—Ä–Ω—É—Ç—å –û–î–ò–ù JSON-–æ–±—ä–µ–∫—Ç:\n"
    "{\n"
    "  \"goal\": <—Å—Ç—Ä–æ–∫–∞>,\n"
    "  \"topic_brief\": <—Å—Ç—Ä–æ–∫–∞>,\n"
    "  \"final_post\": <—Å—Ç—Ä–æ–∫–∞>\n"
    "}\n\n"
    "–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:\n"
    "1) goal ‚Äî –æ–¥–Ω–∞ –∫–æ—Ä–æ—Ç–∫–∞—è —Ñ—Ä–∞–∑–∞, –æ–ø–∏—Å—ã–≤–∞—é—â–∞—è —Ü–µ–ª—å –ø–æ—Å—Ç–∞ (—á—Ç–æ –æ–Ω –¥–æ–ª–∂–µ–Ω —Å–¥–µ–ª–∞—Ç—å –¥–ª—è —á–∏—Ç–∞—Ç–µ–ª—è). "
    "–ë–ï–ó —Å–ª–æ–≤ \"–¶–µ–ª—å –ø–æ—Å—Ç–∞\" –∏ –±–µ–∑ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.\n"
    "2) topic_brief ‚Äî 3‚Äì7 –ª–∞–∫–æ–Ω–∏—á–Ω—ã—Ö –ø—É–Ω–∫—Ç–æ–≤, –∫–∞–∂–¥—ã–π —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏ –∏ —Å –Ω–∞—á–∞–ª–æ–º —á–µ—Ä–µ–∑ –¥–µ—Ñ–∏—Å –∏–ª–∏ —Ç–∏—Ä–µ.\n"
    "   –ó–¥–µ—Å—å –∫—Ä–∞—Ç–∫–æ —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç—Å—è —Ç–µ–º–∞: —á—Ç–æ –∑–∞ —Å–æ–±—ã—Ç–∏–µ / –ø—Ä–æ–¥—É–∫—Ç / –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ, –∫–∞–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ –∞—Å–ø–µ–∫—Ç—ã.\n"
    "   –ù–ï –∫–æ–ø–∏—Ä—É–π –¥–æ—Å–ª–æ–≤–Ω–æ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –ø–æ—Å—Ç–∞ –∏ –Ω–µ –ø–∏—à–∏ —Å—é–¥–∞ —Å–∞–º –ø–æ—Å—Ç.\n"
    "3) final_post ‚Äî –æ—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –ø–æ—Å—Ç–∞ –¥–ª—è Telegram-–∫–∞–Ω–∞–ª–∞.\n"
    "   –ü–∏—à–∏ –≤ –∂–∏–≤–æ–º, –¥–µ–ª–æ–≤–æ–º —Å—Ç–∏–ª–µ, –±–µ–∑ –≤–æ–¥—ã –∏ –∫–ª–∏–∫–±–µ–π—Ç–∞.\n"
    "   –û–±—ä—ë–º final_post ‚Äî –Ω–µ –±–æ–ª–µ–µ 1000‚Äì1200 —Å–∏–º–≤–æ–ª–æ–≤, –Ω–µ –ø—Ä–æ—Å—Ç—ã–Ω—è —Ç–µ–∫—Å—Ç–∞.\n"
    "   –ú–æ–∂–Ω–æ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞—Ç—å –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞—Ç—å –∏—Å—Ö–æ–¥–Ω–∏–∫, –Ω–æ —Å–º—ã—Å–ª –¥–æ–ª–∂–µ–Ω —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å—Å—è.\n\n"
    "–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:\n"
    "- –°—Ç—Ä–æ–≥–æ –æ–¥–∏–Ω –≤–∞–ª–∏–¥–Ω—ã–π JSON-–æ–±—ä–µ–∫—Ç.\n"
    "- –ë–ï–ó –ø–æ—è—Å–Ω–µ–Ω–∏–π –¥–æ –∏–ª–∏ –ø–æ—Å–ª–µ JSON.\n"
    "- –ë–ï–ó –æ–±—ë—Ä—Ç–∫–∏ ```json``` –∏–ª–∏ –ª—é–±—ã—Ö –¥—Ä—É–≥–∏—Ö –∫–æ–¥-–±–ª–æ–∫–æ–≤.\n"
    "- –ë–ï–ó –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –∏ —Ç–µ–∫—Å—Ç–∞ –≤–Ω–µ —Ñ–∏–≥—É—Ä–Ω—ã—Ö —Å–∫–æ–±–æ–∫.\n\n"
    "–ü—Ä–∏–º–µ—Ä –¥–æ–ø—É—Å—Ç–∏–º–æ–≥–æ –æ—Ç–≤–µ—Ç–∞:\n"
    "{ \"goal\": \"–û–±—ä—è—Å–Ω–∏—Ç—å –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ TON\", "
    "\"topic_brief\": \"- –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏\\n- –ù–æ–≤—ã–µ –≥–∞–π–¥—ã\", "
    "\"final_post\": \"–¢–µ–∫—Å—Ç –ø–æ—Å—Ç–∞...\" }\n"
    "–í–ï–†–ù–ò –¢–û–õ–¨–ö–û –û–î–ò–ù –í–ê–õ–ò–î–ù–´–ô JSON. –ù–ò–ß–ï–ì–û –ë–û–õ–¨–®–ï."
)

USER_TEMPLATE = (
    "–ö–∞–Ω–∞–ª: {channel}\n\n"
    "–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø–æ—Å—Ç:\n\"\"\"\n{post}\n\"\"\"\n\n"
    "–°—Ç—Ä–æ–≥–æ —Å–ª–µ–¥—É–π –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏–∑ system-—Å–æ–æ–±—â–µ–Ω–∏—è –∏ –≤–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –æ–¥–∏–Ω JSON-–æ–±—ä–µ–∫—Ç "
    "—Å –ø–æ–ª—è–º–∏ goal, topic_brief, final_post. –ù–∏–∫–∞–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –¥–æ –∏–ª–∏ –ø–æ—Å–ª–µ JSON."
)

# === –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –º–æ–¥–µ–ª—å—é ===

_tokenizer: Any = None
_model: Any = None


def ensure_model():
    """
    –õ–µ–Ω–∏–≤–æ –∑–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä –∏ –º–æ–¥–µ–ª—å –æ–¥–∏–Ω —Ä–∞–∑ –Ω–∞ –ø—Ä–æ—Ü–µ—Å—Å.
    """
    global _tokenizer, _model
    if _tokenizer is not None and _model is not None:
        return

    print(f"[{datetime.now().isoformat()}] –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞–∑–º–µ—Ç–∫–∏ writer_samples...")
    _tokenizer, _model = load_tokenizer_model()

    try:
        device = _model.device
    except Exception:
        params = list(_model.parameters())
        device = params[0].device if params else torch.device("cpu")

    print(f"[{datetime.now().isoformat()}] –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ {device}")


# === JSON-—É—Ç–∏–ª–∏—Ç—ã: —É—Å—Ç–æ–π—á–∏–≤—ã–π –ø–∞—Ä—Å–µ—Ä –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏ ===

def _cut_first_json_block(text: str) -> str:
    """
    –í—ã—Ä–µ–∑–∞–µ–º –ø–µ—Ä–≤—ã–π JSON-–±–ª–æ–∫ –ø–æ –±–∞–ª–∞–Ω—Å—É —Ñ–∏–≥—É—Ä–Ω—ã—Ö —Å–∫–æ–±–æ–∫.
    –ï—Å–ª–∏ –Ω–µ—Ç –∑–∞–∫—Ä—ã–≤–∞—é—â–µ–π '}', –±–µ—Ä—ë–º —Ç–µ–∫—Å—Ç –æ—Ç –ø–µ—Ä–≤–æ–π '{' –¥–æ –∫–æ–Ω—Ü–∞.
    """
    start = text.find("{")
    if start == -1:
        return text

    depth = 0
    end = None
    for i, ch in enumerate(text[start:], start=start):
        if ch == "{":
            depth += 1
        elif ch == "}":
            depth -= 1
            if depth == 0:
                end = i + 1
                break

    if end is not None:
        return text[start:end]
    return text[start:]


def _json_unescape_soft(s: str) -> str:
    """
    –ê–∫–∫—É—Ä–∞—Ç–Ω–æ —Å–Ω–∏–º–∞–µ–º JSON-—ç—Å–∫–µ–π–ø—ã —á–µ—Ä–µ–∑ json.loads,
    –Ω–µ –ª–æ–º–∞—è –∫–∏—Ä–∏–ª–ª–∏—Ü—É –∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É—è unicode_escape.
    """
    try:
        wrapped = '"' + s.replace('\\', '\\\\').replace('"', '\\"') + '"'
        return json.loads(wrapped)
    except Exception:
        return s


def _cut_final_post_tail(raw_tail: str) -> str:
    """
    –£ final_post JSON —á–∞—Å—Ç–æ –Ω–µ –∑–∞–∫—Ä—ã–≤–∞–µ—Ç—Å—è –∫–∞–≤—ã—á–∫–æ–π, –∞ –¥–∞–ª—å—à–µ –∏–¥—ë—Ç –º—É—Å–æ—Ä
    (–∫–∏—Ç–∞–π—Å–∫–∏–π —Ç–µ–∫—Å—Ç, ```json, user –∏ —Ç.–ø.). –ë–µ—Ä—ë–º —Ö–≤–æ—Å—Ç –¥–æ –ø–µ—Ä–≤—ã—Ö
    —Å—Ç–æ–ø-—Å–ª–æ–≤ –∏ —á–∏—Å—Ç–∏–º –∫–æ–Ω—Ü–æ–≤–∫—É –æ—Ç –ª–∏—à–Ω–∏—Ö –∫–∞–≤—ã—á–µ–∫/—Å–∫–æ–±–æ–∫/–∑–∞–ø—è—Ç—ã—Ö.
    """
    stoppers = [
        "\nuser\n",
        "\nuser",
        "\n```",
        "```",
        "ÂØπ‰∏çËµ∑",
        "Ê≥®ÂÜåÁôªÂΩï",
        "Áü•ÊÇâÊÇ®ÁöÑË¶ÅÊ±Ç",
        "Âπ∏Â•ΩÔºåÊÇ®Êèê‰æõÁöÑÁøªËØëÂ∑≤ÁªèÂæàÊé•Ëøë‰∫Ü",
    ]

    end = len(raw_tail)
    for stop in stoppers:
        idx = raw_tail.find(stop)
        if idx != -1 and idx < end:
            end = idx

    s = raw_tail[:end].rstrip()

    # —É–±–∏—Ä–∞–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ –∑–∞–∫—Ä—ã–≤–∞—é—â–∏–µ `"`, `",`, `" }` –∏ —Ç.–ø.
    while s and s[-1] in '" ,}':
        s = s[:-1]

    return s.strip()


def extract_json(text: str) -> Optional[Dict[str, Any]]:
    """
    –£—Å—Ç–æ–π—á–∏–≤–∞—è –≤—ã—Ç—è–∂–∫–∞ JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏.
    """
    if not text:
        return None

    text = re.sub(r"```.*?```", " ", text, flags=re.S)
    text = re.sub(r"[\x00-\x08\x0b-\x0c\x0e-\x1f\x7f-\x9f]", "", text)
    text = text.strip()

    text = _cut_first_json_block(text)

    decoder = json.JSONDecoder()

    # 1) –ü—ã—Ç–∞–µ–º—Å—è –∫–∞–∫ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π JSON
    for m in re.finditer(r"\{", text):
        start = m.start()
        try:
            obj, _ = decoder.raw_decode(text[start:])
            if isinstance(obj, dict):
                goal = _json_unescape_soft(str(obj.get("goal", "") or "")).strip()
                topic_brief = _json_unescape_soft(str(obj.get("topic_brief", "") or "")).strip()
                final_post = _json_unescape_soft(str(obj.get("final_post", "") or "")).strip()

                if not goal and not topic_brief and not final_post:
                    break

                return {
                    "goal": goal,
                    "topic_brief": topic_brief,
                    "final_post": final_post,
                }
        except Exception:
            continue

    # 2) –§–æ–ª–ª–±–µ–∫: goal –∏ topic_brief ‚Äî –æ–±—ã—á–Ω—ã–µ JSON-—Å—Ç—Ä–æ–∫–∏,
    # final_post ‚Äî ¬´—Å–ª–æ–º–∞–Ω–Ω—ã–π¬ª —Ö–≤–æ—Å—Ç –ø–æ—Å–ª–µ –æ—Ç–∫—Ä—ã–≤–∞—é—â–µ–π –∫–∞–≤—ã—á–∫–∏
    goal_match = re.search(r'"goal"\s*:\s*"(.*?)"', text, flags=re.S)
    brief_match = re.search(r'"topic_brief"\s*:\s*"(.*?)"', text, flags=re.S)
    final_match = re.search(r'"final_post"\s*:\s*"(.*)', text, flags=re.S)

    if not (goal_match and brief_match and final_match):
        return None

    goal_raw = goal_match.group(1)
    brief_raw = brief_match.group(1)
    final_tail_raw = final_match.group(1)

    goal = _json_unescape_soft(goal_raw).strip()
    topic_brief = _json_unescape_soft(brief_raw).strip()
    final_post = _cut_final_post_tail(final_tail_raw)

    if not goal and not topic_brief and not final_post:
        return None

    return {
        "goal": goal,
        "topic_brief": topic_brief,
        "final_post": final_post,
    }


def build_messages(channel: str, post_text: str) -> List[Dict[str, str]]:
    return [
        {"role": "system", "content": SYSTEM_MSG},
        {
            "role": "user",
            "content": USER_TEMPLATE.format(channel=channel, post=post_text[:16000]),
        },
    ]


def run_inference(channel: str, post_text: str) -> Optional[Dict[str, Any]]:
    """
    –ü—Ä–æ–≥–æ–Ω –æ–¥–Ω–æ–≥–æ –ø–æ—Å—Ç–∞ —á–µ—Ä–µ–∑ Qwen, –ø–æ–ø—ã—Ç–∫–∞ –≤—ã—Ç–∞—â–∏—Ç—å JSON.
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ–¥–∏–Ω —Ä–∞–∑.
    """
    ensure_model()

    messages = build_messages(channel, post_text)

    try:
        inputs = _tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_tensors="pt",
        )
    except TypeError:
        inputs = _tokenizer.apply_chat_template(
            messages,
            return_tensors="pt",
        )

    if isinstance(inputs, torch.Tensor):
        input_ids = inputs
        attention_mask = None
    elif isinstance(inputs, dict):
        input_ids = inputs.get("input_ids")
        attention_mask = inputs.get("attention_mask")
    else:
        input_ids = inputs
        attention_mask = None

    try:
        device = _model.device
    except Exception:
        params = list(_model.parameters())
        device = params[0].device if params else torch.device("cpu")

    input_ids = input_ids.to(device)

    if attention_mask is None:
        attention_mask = torch.ones_like(input_ids, dtype=torch.long, device=device)
    else:
        attention_mask = attention_mask.to(device)

    gen_kwargs = dict(
        input_ids=input_ids,
        attention_mask=attention_mask,
        max_new_tokens=768,
        do_sample=False,
        pad_token_id=getattr(_tokenizer, "eos_token_id", None),
        eos_token_id=getattr(_tokenizer, "eos_token_id", None),
    )

    with torch.inference_mode():
        out = _model.generate(**gen_kwargs)

    gen_ids = out[0][input_ids.shape[-1]:]
    gen_text = _tokenizer.decode(gen_ids, skip_special_tokens=True)

    js = extract_json(gen_text)
    if js is None:
        print(
            f"[{datetime.now().isoformat()}] ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã—Ç–∞—â–∏—Ç—å JSON –¥–ª—è –∫–∞–Ω–∞–ª–∞ {channel}."
        )
        print("===== RAW gen_text (–ø–æ–ª–Ω—ã–π) =====")
        print(gen_text)
        print("========== END RAW gen_text ==========\n")
    return js


# === –£–∫—Ä–∞—à–µ–Ω–∏–µ –ø–æ—Å—Ç–æ–≤ —ç–º–æ–¥–∑–∏ / ¬´—Å—Ç–∏–∫–µ—Ä–∞–º–∏¬ª ===

def add_emojis(channel: str, text: str) -> str:
    """
    –õ—ë–≥–∫–æ–µ —É–∫—Ä–∞—à–µ–Ω–∏–µ –ø–æ—Å—Ç–æ–≤ —ç–º–æ–¥–∑–∏.
    """
    if not text:
        return text

    text = re.sub(r"\s+", " ", text)

    replacements = {
        "Crypto Pay": "Crypto Pay üí≥",
        "CryptoBot": "CryptoBot ü§ñ",
        "TON ": "TON üíé ",
        " TON": " TON üíé",
        "TON Blockchain": "TON Blockchain üîµ",
        "TON Network": "TON Network üîµ",
        "Telegram": "Telegram ‚úàÔ∏è",
        "USDT": "USDT üíµ",
        "TONüíé": "TON üíé",
        "BTC": "BTC ‚Çø",
        "ETH": "ETH ‚ô¶Ô∏è",
        "SOL": "SOL üü°",
        "LTC": "LTC üåï",
        "TRX": "TRX üî∫",
        "–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç": "–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç ü™ô",
        "–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞": "–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞ ü™ô",
        "–∫—Ä–∏–ø—Ç–æ–π": "–∫—Ä–∏–ø—Ç–æ–π ü™ô",
        "–∫—Ä–∏–ø—Ç–∞": "–∫—Ä–∏–ø—Ç–∞ ü™ô",
        "–∫–æ—à–µ–ª—ë–∫": "–∫–æ—à–µ–ª—ë–∫ üëõ",
        "–∫–æ—à–µ–ª–µ–∫": "–∫–æ—à–µ–ª–µ–∫ üëõ",
        "wallet": "wallet üëõ",
        "—Å—á–µ—Ç–∞": "—Å—á–µ—Ç–∞ üßæ",
        "—Å—á–µ—Ç": "—Å—á—ë—Ç üßæ",
        "—Å—á—ë—Ç": "—Å—á—ë—Ç üßæ",
        "invoice": "invoice üßæ",
        "createInvoice": "createInvoice üßæ",
        "–æ–ø–ª–∞—á–∏–≤–∞—Ç—å": "–æ–ø–ª–∞—á–∏–≤–∞—Ç—å üí∏",
        "–æ–ø–ª–∞—Ç–∞": "–æ–ø–ª–∞—Ç–∞ üí∏",
        "–ø–ª–∞—Ç–µ–∂": "–ø–ª–∞—Ç—ë–∂ üí∏",
        "–ø–ª–∞—Ç—ë–∂": "–ø–ª–∞—Ç—ë–∂ üí∏",
        "–≤—ã–≤–æ–¥–∞ –±–∞–ª–∞–Ω—Å–∞": "–≤—ã–≤–æ–¥–∞ –±–∞–ª–∞–Ω—Å–∞ üîÑ",
        "–±–∞–ª–∞–Ω—Å": "–±–∞–ª–∞–Ω—Å üìä",
        "–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–µ–π": "–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–µ–π üîÅ",
        "–∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–µ–π": "–∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–µ–π üîÅ",
        "–∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏": "–∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ üîÅ",
        "–æ–±–º–µ–Ω": "–æ–±–º–µ–Ω ‚ôªÔ∏è",
        "swap": "swap ‚ôªÔ∏è",
        "swap_to": "swap_to ‚ôªÔ∏è",
        "–æ–±–Ω–æ–≤–ª–µ–Ω–Ω—É—é –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é": "–æ–±–Ω–æ–≤–ª—ë–Ω–Ω—É—é –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é üìò",
        "–æ–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è": "–æ–±–Ω–æ–≤–ª—ë–Ω–Ω—É—é –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é üìò",
        "–æ–±–Ω–æ–≤–ª–µ–Ω–Ω—É—é –¥–æ–∫—É": "–æ–±–Ω–æ–≤–ª—ë–Ω–Ω—É—é –¥–æ–∫—É üìò",
        "–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ": "–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ üöÄ",
        "–Ω–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏": "–Ω–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ‚ú®",
        "–Ω–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è": "–Ω–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è ‚ú®",
        "–Ω–æ–≤—ã–π —Ä–µ–ª–∏–∑": "–Ω–æ–≤—ã–π —Ä–µ–ª–∏–∑ ‚ú®",
        "–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è": "–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è üìò",
        "–≥–∞–π–¥": "–≥–∞–π–¥ üìò",
        "—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ": "—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ üìò",
        "—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∏": "—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∏ üë®‚Äçüíª",
        "—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫": "—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫ üë®‚Äçüíª",
        "–±–æ—Ç–∞—Ö": "–±–æ—Ç–∞—Ö ü§ñ",
        "–±–æ—Ç—ã": "–±–æ—Ç—ã ü§ñ",
        "Mini App": "Mini App üì±",
        "–º–∏–Ω–∏-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏": "–º–∏–Ω–∏-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏üì±",
        "–Ω–∞–ø—Ä–∏–º–µ—Ä,": "–Ω–∞–ø—Ä–∏–º–µ—Ä, üëâ",
        "–¥–ª—è —ç—Ç–æ–≥–æ": "–¥–ª—è —ç—Ç–æ–≥–æ üìå",
        "–ü–æ–º–∏–º–æ —ç—Ç–æ–≥–æ": "–ü–æ–º–∏–º–æ —ç—Ç–æ–≥–æ ‚ûï",
        "–ö—Ä–æ–º–µ —Ç–æ–≥–æ": "–ö—Ä–æ–º–µ —Ç–æ–≥–æ ‚ûï",
        "–º–æ–∂–µ—Ç–µ —É–∫–∞–∑–∞—Ç—å": "–º–æ–∂–µ—Ç–µ —É–∫–∞–∑–∞—Ç—å ‚úçÔ∏è",
        "–º–æ–∂–Ω–æ –∏–∑—É—á–∏—Ç—å": "–º–æ–∂–Ω–æ –∏–∑—É—á–∏—Ç—å üîç",
        "–º–æ–∂–Ω–æ –∏–∑—É—á–∞—Ç—å": "–º–æ–∂–Ω–æ –∏–∑—É—á–∞—Ç—å üîç",
    }

    for src, dst in replacements.items():
        text = text.replace(src, dst)

    return text


# === –†–∞–±–æ—Ç–∞ —Å –ë–î ===

CREATE_WRITER_SAMPLES_SQL = """
CREATE TABLE IF NOT EXISTS writer_samples (
    id SERIAL PRIMARY KEY,
    sample_type VARCHAR(50) NOT NULL DEFAULT 'post',
    source_post_id INTEGER NOT NULL REFERENCES posts(id) ON DELETE CASCADE,
    channel_username VARCHAR(255) NOT NULL,
    goal TEXT NOT NULL,
    topic_brief TEXT NOT NULL,
    final_post TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    gen_status VARCHAR(32) NOT NULL DEFAULT 'ok',
    UNIQUE (sample_type, source_post_id)
);
"""

SELECT_CANDIDATES_SQL = """
SELECT
    p.id AS post_id,
    p.channel_username,
    COALESCE(cp.clean_text, p.post_text) AS text,
    pq.quality_score,
    p.ingest_status
FROM posts p
JOIN post_quality pq
    ON pq.post_id = p.id
LEFT JOIN clean_posts cp
    ON cp.source_post_id = p.id
WHERE
    pq.is_good = true
    AND pq.quality_score >= $1
    AND p.ingest_status = 'done'
    AND COALESCE(cp.clean_text, p.post_text) IS NOT NULL
    AND TRIM(COALESCE(cp.clean_text, p.post_text)) <> ''
    AND NOT EXISTS (
        SELECT 1 FROM writer_samples ws
        WHERE ws.source_post_id = p.id
          AND ws.sample_type = 'post'
    )
ORDER BY pq.quality_score DESC, p.id
LIMIT $2;
"""

INSERT_WRITER_SAMPLE_SQL = """
INSERT INTO writer_samples (
    sample_type,
    source_post_id,
    channel_username,
    goal,
    topic_brief,
    final_post,
    gen_status
) VALUES ($1, $2, $3, $4, $5, $6, $7);
"""


async def ensure_writer_samples_table(conn: asyncpg.Connection) -> None:
    """
    –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º, —á—Ç–æ writer_samples —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ –≤ –Ω–µ–π –µ—Å—Ç—å gen_status.
    """
    await conn.execute(CREATE_WRITER_SAMPLES_SQL)
    # –ù–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ —Ç–∞–±–ª–∏—Ü–∞ —Å–æ–∑–¥–∞–≤–∞–ª–∞—Å—å —Å—Ç–∞—Ä–æ–π –≤–µ—Ä—Å–∏–µ–π –±–µ–∑ gen_status
    await conn.execute(
        "ALTER TABLE writer_samples "
        "ADD COLUMN IF NOT EXISTS gen_status VARCHAR(32) NOT NULL DEFAULT 'ok';"
    )


async def fetch_candidates(conn) -> List[asyncpg.Record]:
    rows = await conn.fetch(SELECT_CANDIDATES_SQL, MIN_QUALITY_SCORE, MAX_POSTS)
    print(f"[{datetime.now().isoformat()}] –ù–∞–π–¥–µ–Ω–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è —Ä–∞–∑–º–µ—Ç–∫–∏: {len(rows)}")
    return rows


async def save_writer_sample(
    conn: asyncpg.Connection,
    post_id: int,
    channel: str,
    goal: str,
    topic_brief: str,
    final_post: str,
    gen_status: str = "ok",
) -> None:
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ä–∞–∑–º–µ—Ç–∫–∏. gen_status:
    - 'ok'    ‚Äî –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π —Å—ç–º–ø–ª
    - 'error' ‚Äî –º–æ–¥–µ–ª—å –Ω–µ —Å–º–æ–≥–ª–∞ —Å–≥–µ–Ω–µ—Ä–∏—Ç—å –∞–¥–µ–∫–≤–∞—Ç–Ω—ã–π JSON
    """
    await conn.execute(
        INSERT_WRITER_SAMPLE_SQL,
        "post",
        post_id,
        channel,
        goal.strip(),
        topic_brief.strip(),
        final_post.strip(),
        gen_status,
    )


def print_progress(current: int, total: int) -> None:
    """
    –ö—Ä–∞—Å–∏–≤—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –≤ –∫–æ–Ω—Å–æ–ª–∏.

    current ‚Äî —Å–∫–æ–ª—å–∫–æ –ø–æ—Å—Ç–æ–≤ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ (—É—Å–ø–µ—à–Ω–æ –∏–ª–∏ –ø–æ–º–µ—á–µ–Ω–æ error),
    total ‚Äî –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤.
    """
    if total <= 0:
        return

    # –≤—ã–≤–æ–¥–∏–º —Ç–æ–ª—å–∫–æ –∏–Ω–æ–≥–¥–∞, —á—Ç–æ–±—ã –Ω–µ —Å–ø–∞–º–∏—Ç—å
    if current != total and current % 50 != 0:
        return

    ratio = current / total
    bar_len = 30
    filled = int(bar_len * ratio)
    bar = "‚ñà" * filled + "‚ñë" * (bar_len - filled)

    print(
        f"[{datetime.now().isoformat()}] –ü—Ä–æ–≥—Ä–µ—Å—Å —Ä–∞–∑–º–µ—Ç–∫–∏: "
        f"|{bar}| {ratio * 100:5.1f}% ({current}/{total})",
        flush=True,
    )


# === –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª ===

async def main():
    print(f"[{datetime.now().isoformat()}] üöÄ –ê–≤—Ç–æ—Ä–∞–∑–º–µ—Ç–∫–∞ writer_samples —Å—Ç–∞—Ä—Ç—É–µ—Ç...")
    conn = await asyncpg.connect(**DB)
    try:
        await ensure_writer_samples_table(conn)

        rows = await fetch_candidates(conn)
        total = len(rows)
        if not rows:
            print(f"[{datetime.now().isoformat()}] –ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –ø–æ—Å—Ç–æ–≤ ‚Äî –≤—ã—Ö–æ–¥–∏–º.")
            return

        processed_ok = 0
        processed_error = 0
        seen = 0

        for r in rows:
            seen += 1
            post_id = r["post_id"]
            channel = r["channel_username"]
            text = (r["text"] or "").strip()
            ingest_status = r["ingest_status"]

            if ingest_status != "done":
                # –ª–æ–≥–∏—á–µ—Å–∫–∏ —Å—é–¥–∞ –ø–æ—á—Ç–∏ –Ω–µ –ø–æ–ø–∞–¥—ë–º –∏–∑-–∑–∞ WHERE, –Ω–æ –ø—É—Å—Ç—å –±—É–¥–µ—Ç
                print(
                    f"[{datetime.now().isoformat()}] ‚ö†Ô∏è post_id={post_id} —Å ingest_status={ingest_status}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –±–µ–∑ –∑–∞–ø–∏—Å–∏."
                )
                print_progress(seen, total)
                continue

            if not text:
                print(
                    f"[{datetime.now().isoformat()}] ‚ö†Ô∏è –ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –¥–ª—è post_id={post_id}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –±–µ–∑ –∑–∞–ø–∏—Å–∏."
                )
                print_progress(seen, total)
                continue

            print(
                f"[{datetime.now().isoformat()}] ‚Üí –û–±—Ä–∞–±–æ—Ç–∫–∞ post_id={post_id} ({channel}), ingest_status={ingest_status}"
            )

            js = run_inference(channel, text)
            if not js:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç—Ä–æ–∫—É —Å gen_status='error', —á—Ç–æ–±—ã –±–æ–ª—å—à–µ –Ω–µ —Ç—Ä–æ–≥–∞—Ç—å —ç—Ç–æ—Ç –ø–æ—Å—Ç
                await save_writer_sample(
                    conn,
                    post_id,
                    channel,
                    goal="[error]",
                    topic_brief="[error]",
                    final_post="[error]",
                    gen_status="error",
                )
                processed_error += 1
                print(
                    f"[{datetime.now().isoformat()}] ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã—Ç–∞—â–∏—Ç—å JSON –¥–ª—è post_id={post_id}, –ø–æ–º–µ—Ç–∏–ª–∏ gen_status='error'."
                )
                print_progress(seen, total)
                continue

            goal = str(js.get("goal", "") or "").strip()
            topic_brief = str(js.get("topic_brief", "") or "").strip()
            final_post = str(js.get("final_post", "") or "").strip()

            if not goal or not topic_brief or not final_post:
                await save_writer_sample(
                    conn,
                    post_id,
                    channel,
                    goal or "[error]",
                    topic_brief or "[error]",
                    final_post or "[error]",
                    gen_status="error",
                )
                processed_error += 1
                print(
                    f"[{datetime.now().isoformat()}] ‚ö†Ô∏è –ü—É—Å—Ç—ã–µ –ø–æ–ª—è –≤ JSON –¥–ª—è post_id={post_id}, –ø–æ–º–µ—Ç–∏–ª–∏ gen_status='error'."
                )
                print_progress(seen, total)
                continue

            # –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π –∫–µ–π—Å
            final_post = add_emojis(channel, final_post)

            await save_writer_sample(
                conn,
                post_id,
                channel,
                goal,
                topic_brief,
                final_post,
                gen_status="ok",
            )
            processed_ok += 1
            print(
                f"[{datetime.now().isoformat()}] ‚úÖ post_id={post_id} ‚Üí –∑–∞–ø–∏—Å–∞–Ω –≤ writer_samples (gen_status='ok')"
            )

            print_progress(seen, total)

        print()  # –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏ –ø–æ—Å–ª–µ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞

        print(
            f"[{datetime.now().isoformat()}] –ì–æ—Ç–æ–≤–æ. –£—Å–ø–µ—à–Ω–æ (ok): {processed_ok}, —Å –æ—à–∏–±–∫–æ–π (error): {processed_error}"
        )

    finally:
        await conn.close()
        print(f"[{datetime.now().isoformat()}] üîå –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –ë–î –∑–∞–∫—Ä—ã—Ç–æ.")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("–û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.")
