# analytics/judge_quality_llm.py
# Production-ready runner for post quality judging:
# - atomic batch fetch (safe for concurrent workers)
# - robust tokenization normalization and device placement
# - JSON repair + secondary model-based extraction
# - retries, attempts counting, fallback heuristic
# - progress printing and simple GPU memory info
# - records signals with raw_output, metrics, inference_time

from __future__ import annotations
import os
import sys
import json
import time
import re
import asyncio
import inspect
import warnings
from typing import List, Dict, Any
from datetime import datetime

import asyncpg
import torch
from dotenv import load_dotenv

# Project base
import pathlib
BASE_DIR = str(pathlib.Path(__file__).resolve().parents[1])
if BASE_DIR not in sys.path:
    sys.path.insert(0, BASE_DIR)

# Local model loader (assumed present)
from Models.qwen_loader import load_tokenizer_model

# optional transformers GenerationConfig
try:
    from transformers import GenerationConfig, logging as transformers_logging
except Exception:
    GenerationConfig = None
    transformers_logging = None

# load env
load_dotenv(os.path.join(BASE_DIR, ".env"))

# configuration
DB = {
    "host": os.getenv("POSTGRES_HOST", "127.0.0.1"),
    "port": int(os.getenv("POSTGRES_PORT", 5432)),
    "database": os.getenv("POSTGRES_DB", "engagex"),
    "user": os.getenv("POSTGRES_USER", "engagex"),
    "password": os.getenv("POSTGRES_PASSWORD", "engagex"),
}

JUDGE_BATCH = int(os.getenv("JUDGE_BATCH", "32"))
MODEL_VERSION = os.getenv("MODEL_VERSION", "qwen-local-v1")
SAMPLE_MODE = os.getenv("SAMPLE_MODE", "0")
MAX_NEW_TOKENS = int(os.getenv("MAX_NEW_TOKENS", "512"))

# globals for model
_tokenizer = None
_model = None


def ensure_model():
    global _tokenizer, _model
    if _tokenizer is None or _model is None:
        print(f"[{datetime.now().isoformat()}] Loading tokenizer+model...")
        _tokenizer, _model = load_tokenizer_model()
        try:
            device = _model.device
        except Exception:
            params = list(_model.parameters())
            device = params[0].device if params else torch.device("cpu")
        print(f"[{datetime.now().isoformat()}] Model loaded on device {device}")

        # ensure pad token so tokenizer can build attention_mask if needed
        if getattr(_tokenizer, "pad_token_id", None) is None:
            try:
                _tokenizer.add_special_tokens({"pad_token": "[PAD]"})
                _model.resize_token_embeddings(len(_tokenizer))
            except Exception as e:
                warnings.warn(f"Could not add pad_token to tokenizer: {e}")


# prompts
SYSTEM_MSG = (
    "–¢—ã ‚Äî —Å—Ç—Ä–æ–≥–∏–π, –Ω–æ –ø—Ä–æ—Å—Ç–æ–π –º–æ–¥–µ—Ä–∞—Ç–æ—Ä. "
    "–¢–≤–æ—è –ï–î–ò–ù–°–¢–í–ï–ù–ù–ê–Ø –∑–∞–¥–∞—á–∞ ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –ª–∏ –ø–æ—Å—Ç –∫ —Ç–µ–º–∞—Ç–∏–∫–µ —Ñ–∏—Ç–Ω–µ—Å–∞, "
    "—Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ–∫ –∏ –∑–¥–æ—Ä–æ–≤–æ–≥–æ –æ–±—Ä–∞–∑–∞ –∂–∏–∑–Ω–∏.\n"
    "\n"
    "–°—á–∏—Ç–∞–π –ø–æ—Å—Ç ¬´–ø–æ —Ç–µ–º–µ¬ª, –µ—Å–ª–∏ –æ–Ω –ø—Ä–æ –æ–¥–Ω–æ –∏–∑ —Å–ª–µ–¥—É—é—â–µ–≥–æ (–Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –≤—Å—ë —Å—Ä–∞–∑—É):\n"
    "- —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è, —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏, –ø–ª–∞–Ω—ã —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ–∫, –∫–æ–º–ø–ª–µ–∫—Å—ã —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–π "
    "(–≤ —Ç–æ–º —á–∏—Å–ª–µ –∫–æ—Ä–æ—Ç–∫–∏–µ, —Ç–∏–ø–∞ ¬´—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –ø—Ä–µ—Å—Å–∞ –∑–∞ 7 –º–∏–Ω—É—Ç¬ª);\n"
    "- –º–æ—Ç–∏–≤–∞—Ü–∏—è –∫ –∑–∞–Ω—è—Ç–∏—è–º —Å–ø–æ—Ä—Ç–æ–º, –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞—Ö;\n"
    "- –ø–∏—Ç–∞–Ω–∏–µ, –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ, —Å–æ–Ω –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Ñ–æ—Ä–º—ã –∏ –∑–¥–æ—Ä–æ–≤—å—è;\n"
    "- –ª—é–±—ã–µ —Å–æ–≤–µ—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–º–æ–≥–∞—é—Ç —á–µ–ª–æ–≤–µ–∫—É –ª—É—á—à–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞—Ç—å—Å—è –∏–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å —Ñ–æ—Ä–º—É.\n"
    "\n"
    "–ï—Å–ª–∏ –ø–æ—Å—Ç –ø–æ —Ç–µ–º–µ ‚Äî —Å—Ç–∞–≤—å is_good=true –∏ score=100.\n"
    "–ï—Å–ª–∏ –ø–æ—Å—Ç –ù–ï –ø–æ —Ç–µ–º–µ (–ø–æ–ª–∏—Ç–∏–∫–∞, –Ω–æ–≤–æ—Å—Ç–∏, –º–µ–º—ã, –∫—Ä–∏–ø—Ç–∞, –±–∏–∑–Ω–µ—Å –Ω–∏ —Ä–∞–∑—É –Ω–µ –ø—Ä–æ —Ñ–∏—Ç–Ω–µ—Å –∏ —Ç.–ø.) ‚Äî "
    "—Å—Ç–∞–≤—å is_good=false –∏ score=0.\n"
    "\n"
    "–ú–æ–∂–µ—à—å –∫—Ä–∞—Ç–∫–æ –æ–±—ä—è—Å–Ω–∏—Ç—å —Ä–µ—à–µ–Ω–∏–µ –≤ –º–∞—Å—Å–∏–≤–µ reasons.\n"
    "–ü–æ–ª—è labels –∑–∞–ø–æ–ª–Ω–∏ –ª—é–±—ã–º–∏ —á–∏—Å–ª–∞–º–∏ –æ—Ç 0 –¥–æ 100 –ø–æ —Å–≤–æ–µ–º—É —É—Å–º–æ—Ç—Ä–µ–Ω–∏—é, –≥–ª–∞–≤–Ω–æ–µ ‚Äî —á—Ç–æ–±—ã JSON –±—ã–ª –≤–∞–ª–∏–¥–Ω—ã–π.\n"
    "\n"
    "–í–ï–†–ù–ò –¢–û–õ–¨–ö–û –û–î–ò–ù –í–ê–õ–ò–î–ù–´–ô JSON. –ù–ò–ß–ï–ì–û –ë–û–õ–¨–®–ï.\n"
    "–§–æ—Ä–º–∞—Ç JSON: {\"score\": <0..100>, \"is_good\": <true|false>, \"reasons\": [..], "
    "\"labels\": {\"clarity\":..,\"usefulness\":..,\"engagement\":..,\"ethics\":..}}\n"
)

PROMPT_USER_TEMPLATE = (
    "POST_ID: {post_id}\nCHANNEL: {channel}\n"
    "METRICS: views={views}, forwards={forwards}, reactions={reactions}, "
    "comments={comments}, engagement_rate={engagement_rate}\n"
    "POST:\n\"\"\"\n{post}\n\"\"\"\n\n"
    "–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ."
)


def build_messages(text: str, post_id: int, channel: str, metrics: Dict[str, Any]):
    m = PROMPT_USER_TEMPLATE.format(
        post_id=post_id,
        channel=channel,
        views=metrics.get("views", 0),
        forwards=metrics.get("forwards", 0),
        reactions=metrics.get("reactions_sum", 0),
        comments=metrics.get("comments_count", 0),
        engagement_rate=metrics.get("engagement_rate", 0.0),
        post=text[:16000],
    )
    return [
        {"role": "system", "content": SYSTEM_MSG},
        {"role": "user", "content": m},
    ]


# ------------------- tokenization normalization -------------------
def _normalize_input_bundle(input_bundle):
    if isinstance(input_bundle, torch.Tensor):
        return {"input_ids": input_bundle}
    if isinstance(input_bundle, dict):
        return input_bundle
    if isinstance(input_bundle, (list, tuple)):
        tensors = [x for x in input_bundle if isinstance(x, torch.Tensor)]
        if len(tensors) == 1:
            return {"input_ids": tensors[0]}
        if len(tensors) >= 2:
            return {"input_ids": tensors[0], "attention_mask": tensors[1]}
        for x in input_bundle:
            if isinstance(x, dict):
                return x
        raise RuntimeError(f"Unsupported tokenizer return tuple shape: {type(input_bundle)}")
    try:
        attrs = vars(input_bundle)
        d = {k: v for k, v in attrs.items() if isinstance(v, torch.Tensor)}
        if d:
            return d
    except Exception:
        pass
    raise RuntimeError(f"Unsupported tokenizer return type: {type(input_bundle)}")


def _to_device_and_prepare(input_dict, device):
    new = {}
    for k, v in input_dict.items():
        if isinstance(v, torch.Tensor):
            new[k] = v.to(device)
        else:
            try:
                new[k] = torch.tensor(v, device=device)
            except Exception:
                pass
    if "input_ids" not in new:
        raise RuntimeError("Tokenized output has no 'input_ids'")
    if "attention_mask" not in new:
        new["attention_mask"] = torch.ones_like(new["input_ids"], dtype=torch.long, device=device)
    return new


def _supports_generation_config():
    try:
        sig = inspect.signature(_model.generate)
        return "generation_config" in sig.parameters
    except Exception:
        return False


# ------------------- JSON extract & repair -------------------
def try_find_json_with_decoder(text: str):
    decoder = json.JSONDecoder()
    for m in re.finditer(r"\{", text):
        start = m.start()
        try:
            obj, idx = decoder.raw_decode(text[start:])
            return obj
        except Exception:
            continue
    return None


def repair_json_text(gen_text: str):
    s = gen_text
    # remove fenced code blocks
    s = re.sub(r"```.*?```", " ", s, flags=re.S)
    s = s.replace("`", " ")
    # smart quotes -> normal
    s = (
        s.replace("‚Äú", '"')
        .replace("‚Äù", '"')
        .replace("¬´", '"')
        .replace("¬ª", '"')
        .replace("‚Äô", "'")
    )
    # remove control chars
    s = re.sub(r"[\x00-\x08\x0b-\x0c\x0e-\x1f\x7f-\x9f]", "", s)
    # try to find JSON-like chunks
    for m in re.finditer(r"\{", s):
        start = m.start()
        chunk = s[start:]
        # attempt to close at last brace
        last = chunk.rfind("}")
        if last != -1:
            candidate = chunk[: last + 1]
        else:
            candidate = chunk
        candidate = candidate.replace("\n", " ")
        candidate = re.sub(r",\s*}", "}", candidate)
        candidate = re.sub(r",\s*\]", "]", candidate)
        candidate = re.sub(r"\s+", " ", candidate).strip()
        try:
            return json.loads(candidate)
        except Exception:
            continue
    return None


def extract_or_recover_json(gen_text: str):
    parsed = try_find_json_with_decoder(gen_text)
    if parsed is not None:
        return parsed
    repaired = repair_json_text(gen_text)
    if repaired is not None:
        return repaired
    return None


# ------------------- fallback heuristic -------------------
def heuristic_fallback_score(metrics: Dict[str, Any]) -> int:
    er = metrics.get("engagement_rate", 0.0)
    views = metrics.get("views", 0)
    if views >= 500 or er > 0.05:
        return 80
    if views >= 100 or er > 0.02:
        return 55
    if views < 10:
        return 10
    return 35


# ------------------- secondary extraction using model -------------------
def extract_with_model(raw_output: str):
    # secondary prompt: ask the model to return a JSON only
    prompt = [
        {
            "role": "system",
            "content": "–¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫. –ò–∑–≤–ª–µ–∫–∏ –æ–¥–∏–Ω –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π JSON-–æ–±—ä–µ–∫—Ç –∏–∑ –¥–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞. –ù–∏—á–µ–≥–æ –∫—Ä–æ–º–µ JSON.",
        },
        {
            "role": "user",
            "content": "–¢–µ–∫—Å—Ç:\n\"\"\"\n"
            + raw_output[:16000]
            + "\n\"\"\"\n\n–í–µ—Ä–Ω–∏ –û–î–ò–ù JSON.",
        },
    ]
    try:
        inb = _tokenizer.apply_chat_template(
            prompt, add_generation_prompt=False, return_tensors="pt"
        )
    except TypeError:
        inb = _tokenizer.apply_chat_template(prompt, return_tensors="pt")
    normalized = _normalize_input_bundle(inb)
    device = _model.device if hasattr(_model, "device") else torch.device("cpu")
    input_dict = _to_device_and_prepare(normalized, device)
    try:
        with torch.inference_mode():
            out = _model.generate(
                input_ids=input_dict["input_ids"],
                attention_mask=input_dict["attention_mask"],
                max_new_tokens=200,
                do_sample=False,
                pad_token_id=getattr(_tokenizer, "eos_token_id", None),
                eos_token_id=getattr(_tokenizer, "eos_token_id", None),
            )
        start = input_dict["input_ids"].shape[-1]
        gen_ids = out[0][start:]
        gen_text = _tokenizer.decode(gen_ids, skip_special_tokens=True)
        return extract_or_recover_json(gen_text)
    except Exception:
        return None


# ------------------- inference over items -------------------
def infer_batch(items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    items: [{'post_id':int,'channel':str,'text':str,'metrics':{...}}...]
    """
    ensure_model()
    results = []
    try:
        device = _model.device
    except Exception:
        params = list(_model.parameters())
        device = params[0].device if params else torch.device("cpu")

    use_generation_config = _supports_generation_config() and (GenerationConfig is not None)

    total = len(items)
    last_time = None
    for i, it in enumerate(items, start=1):
        post_id = it["post_id"]
        text = it["text"]
        channel = it["channel"]
        metrics = it.get("metrics", {})
        # progress print
        now = time.time()
        avg = (now - last_time) if last_time else 0.0
        last_time = now
        print(
            f"[{datetime.now().isoformat()}] LLM infer: {i}/{total} post_id={post_id} avg_last={avg:.2f}s",
            end="\r",
            flush=True,
        )

        messages = build_messages(text, post_id, channel, metrics)
        # tokenization
        try:
            inb = _tokenizer.apply_chat_template(
                messages, add_generation_prompt=True, return_tensors="pt"
            )
        except TypeError:
            inb = _tokenizer.apply_chat_template(messages, return_tensors="pt")

        try:
            normalized = _normalize_input_bundle(inb)
            input_dict = _to_device_and_prepare(normalized, device)
        except Exception as e:
            warnings.warn(f"Tokenization failed for post {post_id}: {e}")
            fb = heuristic_fallback_score(metrics)
            results.append(
                {
                    "score": fb,
                    "is_good": fb >= 50,
                    "reasons": ["tokenization_failed_fallback"],
                    "labels": {
                        "clarity": 0,
                        "usefulness": 0,
                        "engagement": 0,
                        "ethics": 0,
                    },
                    "raw_output": "",
                    "inference_time_s": 0.0,
                }
            )
            continue

        gen_kwargs = dict(
            input_ids=input_dict["input_ids"],
            attention_mask=input_dict["attention_mask"],
            max_new_tokens=MAX_NEW_TOKENS,
            pad_token_id=getattr(_tokenizer, "eos_token_id", None),
            eos_token_id=getattr(_tokenizer, "eos_token_id", None),
            do_sample=False,
        )

        if use_generation_config and SAMPLE_MODE == "1":
            temp = float(os.getenv("SAMPLE_TEMPERATURE", 0.7))
            top_p = float(os.getenv("SAMPLE_TOP_P", 0.9))
            top_k = int(os.getenv("SAMPLE_TOP_K", 50))
            gen_cfg = GenerationConfig(
                max_new_tokens=MAX_NEW_TOKENS,
                do_sample=True,
                temperature=temp,
                top_p=top_p,
                top_k=top_k,
            )
            gen_kwargs = {
                "input_ids": input_dict["input_ids"],
                "attention_mask": input_dict["attention_mask"],
                "generation_config": gen_cfg,
                "pad_token_id": gen_cfg.pad_token_id or gen_kwargs["pad_token_id"],
                "eos_token_id": gen_kwargs["eos_token_id"],
            }

        # generation
        t0 = time.time()
        try:
            with torch.inference_mode():
                out = _model.generate(**gen_kwargs)
        except TypeError as e:
            warnings.warn(f"generate TypeError for post {post_id}: {e}; retrying minimal")
            try:
                out = _model.generate(
                    input_ids=input_dict["input_ids"],
                    attention_mask=input_dict["attention_mask"],
                    max_new_tokens=MAX_NEW_TOKENS,
                    do_sample=False,
                    pad_token_id=getattr(_tokenizer, "eos_token_id", None),
                    eos_token_id=getattr(_tokenizer, "eos_token_id", None),
                )
            except Exception as e2:
                warnings.warn(f"generate failed for post {post_id}: {e2}")
                fb = heuristic_fallback_score(metrics)
                results.append(
                    {
                        "score": fb,
                        "is_good": fb >= 50,
                        "reasons": ["generation_failed_fallback"],
                        "labels": {
                            "clarity": 0,
                            "usefulness": 0,
                            "engagement": 0,
                            "ethics": 0,
                        },
                        "raw_output": "",
                        "inference_time_s": 0.0,
                    }
                )
                continue
        except Exception as e:
            warnings.warn(f"Generation exception for post {post_id}: {e}")
            fb = heuristic_fallback_score(metrics)
            results.append(
                {
                    "score": fb,
                    "is_good": fb >= 50,
                    "reasons": ["generation_exception_fallback"],
                    "labels": {
                        "clarity": 0,
                        "usefulness": 0,
                        "engagement": 0,
                        "ethics": 0,
                    },
                    "raw_output": "",
                    "inference_time_s": 0.0,
                }
            )
            continue
        t1 = time.time()
        inference_time = t1 - t0

        # decode generated part
        try:
            out_ids = out[0] if not isinstance(out, list) else out[0]
            start = input_dict["input_ids"].shape[-1]
            gen_ids = out_ids[start:]
            gen_text = _tokenizer.decode(gen_ids, skip_special_tokens=True)
        except Exception as e:
            warnings.warn(f"Decoding failed for post {post_id}: {e}")
            gen_text = ""

        # extract JSON
        js = extract_or_recover_json(gen_text)
        raw_out = gen_text
        reason_tag = None

        if js is None:
            # try secondary extraction with model itself
            js = extract_with_model(gen_text)
            if js is not None:
                reason_tag = "recovered_by_model"
            else:
                reason_tag = "bad_json"

        if js is None:
            fb = heuristic_fallback_score(metrics)
            results.append(
                {
                    "score": fb,
                    "is_good": fb >= 50,
                    "reasons": ["bad_json_fallback"],
                    "labels": {
                        "clarity": 0,
                        "usefulness": 0,
                        "engagement": 0,
                        "ethics": 0,
                    },
                    "raw_output": (raw_out[:2000] if raw_out else ""),
                    "inference_time_s": inference_time,
                }
            )
            continue

        # parse js fields
        try:
            is_good = bool(js.get("is_good", False))
            reasons = js.get("reasons", [])
            labels = js.get("labels", {})

            # –±–∏–Ω–∞—Ä–Ω—ã–π —Å–∫–æ—Ä: —Ç–æ–ª—å–∫–æ –ø–æ —Ñ–∞–∫—Ç—É ¬´–ø–æ —Ç–µ–º–µ / –Ω–µ –ø–æ —Ç–µ–º–µ¬ª
            score = 100.0 if is_good else 0.0

            entry_reasons = reasons[:6] if isinstance(reasons, list) else [str(reasons)]
            if reason_tag:
                entry_reasons.append(reason_tag)

            entry = {
                "score": score,
                "is_good": is_good,
                "reasons": entry_reasons,
                "labels": {
                    "clarity": float(labels.get("clarity", 0)),
                    "usefulness": float(labels.get("usefulness", 0)),
                    "engagement": float(labels.get("engagement", 0)),
                    "ethics": float(labels.get("ethics", 0)),
                },
                "raw_output": (raw_out[:2000] if raw_out else ""),
                "inference_time_s": inference_time,
            }
            results.append(entry)
        except Exception as e:
            warnings.warn(f"Failed to parse js for post {post_id}: {e}")
            fb = heuristic_fallback_score(metrics)
            results.append(
                {
                    "score": fb,
                    "is_good": fb >= 50,
                    "reasons": ["bad_json_parse_fallback"],
                    "labels": {
                        "clarity": 0,
                        "usefulness": 0,
                        "engagement": 0,
                        "ethics": 0,
                    },
                    "raw_output": (raw_out[:2000] if raw_out else ""),
                    "inference_time_s": inference_time,
                }
            )
            continue

    print()  # newline after progress line
    # show GPU mem if available
    try:
        if torch.cuda.is_available():
            d = _model.device
            used = torch.cuda.memory_allocated(d) / 1024**2
            reserved = torch.cuda.memory_reserved(d) / 1024**2
            print(
                f"[{datetime.now().isoformat()}] GPU: {d} used={used:.0f}MiB reserved={reserved:.0f}MiB"
            )
    except Exception:
        pass

    return results


# ------------------- DB helpers for atomic batches -------------------
async def atomic_fetch_and_mark(conn: asyncpg.Connection, batch: int, pid: int):
    """
    –í—ã–±–∏—Ä–∞–µ–º –±–∞—Ç—á –ø–æ—Å—Ç–æ–≤, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –µ—â—ë –Ω–µ—Ç –∑–∞–ø–∏—Å–∏ –≤ post_quality.
    –ó–∞—â–∏—Ç–∞ –æ—Ç –≥–æ–Ω–æ–∫ ‚Äî —á–µ—Ä–µ–∑ FOR UPDATE SKIP LOCKED.
    """
    rows = await conn.fetch(
        """
        SELECT p.id
        FROM posts p
        WHERE NOT EXISTS (
            SELECT 1
            FROM post_quality pq
            WHERE pq.post_id = p.id
        )
        ORDER BY p.id
        LIMIT $1
        FOR UPDATE SKIP LOCKED
        """,
        batch,
    )
    if not rows:
        return []

    ids = [r["id"] for r in rows]

    fetch_sql = """
    SELECT p.id,
           p.channel_username,
           COALESCE(cp.clean_text, p.post_text) AS text,
           p.views,
           p.forwards
    FROM posts p
    LEFT JOIN clean_posts cp ON cp.source_post_id = p.id
    WHERE p.id = ANY($1::int[])
    ORDER BY p.id
    """
    rows2 = await conn.fetch(fetch_sql, ids)

    # aggregate reactions and comments for these ids
    reactions = {}
    rows_r = await conn.fetch(
        "SELECT post_id, SUM(reaction_count) AS reactions_sum "
        "FROM reactions WHERE post_id = ANY($1::int[]) GROUP BY post_id",
        ids,
    )
    for r in rows_r:
        reactions[r["post_id"]] = int(r["reactions_sum"] or 0)

    comments = {}
    rows_c = await conn.fetch(
        "SELECT post_id, COUNT(*) AS comments_count "
        "FROM comments WHERE post_id = ANY($1::int[]) GROUP BY post_id",
        ids,
    )
    for r in rows_c:
        comments[r["post_id"]] = int(r["comments_count"] or 0)

    result = []
    for r in rows2:
        pid_row = int(r["id"])
        views = int(r["views"] or 0)
        forwards = int(r["forwards"] or 0)
        reactions_sum = reactions.get(pid_row, 0)
        comments_count = comments.get(pid_row, 0)
        engagement_rate = (
            (reactions_sum + comments_count) / max(1, views) if views > 0 else 0.0
        )
        result.append(
            {
                "id": pid_row,
                "channel_username": r["channel_username"],
                "text": (r["text"] or "").strip() or " ",
                "views": views,
                "forwards": forwards,
                "reactions_sum": reactions_sum,
                "comments_count": comments_count,
                "engagement_rate": round(engagement_rate, 6),
            }
        )
    return result


# UPSERT for results
# –í–ê–ñ–ù–û: –µ—Å—Ç—å –∫–æ–ª–æ–Ω–∫–∞ gen_status (VARCHAR(32)), –∫–æ—Ç–æ—Ä—É—é –∫–æ–¥ —è–≤–Ω–æ –ø—Ä–æ—Å—Ç–∞–≤–ª—è–µ—Ç.
UPSERT_SQL = """
INSERT INTO post_quality (
    post_id,
    channel_username,
    quality_score,
    is_good,
    signals,
    gen_status,
    updated_at
)
VALUES ($1, $2, $3, $4, $5::jsonb, $6, now())
ON CONFLICT (post_id) DO UPDATE
SET quality_score = EXCLUDED.quality_score,
    is_good       = EXCLUDED.is_good,
    signals       = EXCLUDED.signals,
    gen_status    = EXCLUDED.gen_status,
    updated_at    = now();
"""


# ------------------- main loop -------------------
async def main():
    print(f"[{datetime.now().isoformat()}] üßë‚Äç‚öñÔ∏è LLM-–æ—Ü–µ–Ω–∫–∞ –ø–æ—Å—Ç–æ–≤ ‚Üí post_quality")
    ensure_model()

    conn = await asyncpg.connect(**DB)
    try:
        # —Å–∫–æ–ª—å–∫–æ –ø–æ—Å—Ç–æ–≤ –µ—â—ë –Ω–µ –æ—Ü–µ–Ω–µ–Ω–æ
        total_planned = await conn.fetchval(
            """
            SELECT COUNT(*)
            FROM posts p
            WHERE NOT EXISTS (
                SELECT 1
                FROM post_quality pq
                WHERE pq.post_id = p.id
            )
            """
        )
        print(
            f"[{datetime.now().isoformat()}] üìä –ù–∞–π–¥–µ–Ω–æ {total_planned} –ø–æ—Å—Ç–æ–≤ –±–µ–∑ –æ—Ü–µ–Ω–∫–∏. "
            f"–ë—É–¥–µ–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –±–∞—Ç—á–∞–º–∏ –ø–æ {JUDGE_BATCH}."
        )

        pid = os.getpid()
        total = 0

        while True:
            async with conn.transaction():
                items = await atomic_fetch_and_mark(conn, JUDGE_BATCH, pid)

            if not items:
                # –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –≤–¥—Ä—É–≥ –ø–æ–∫–∞ —Ä–∞–±–æ—Ç–∞–ª–∏, –¥–æ–±–∞–≤–∏–ª–∏—Å—å –ø–æ—Å—Ç—ã
                remaining = await conn.fetchval(
                    """
                    SELECT COUNT(*)
                    FROM posts p
                    WHERE NOT EXISTS (
                        SELECT 1
                        FROM post_quality pq
                        WHERE pq.post_id = p.id
                    )
                    """
                )
                if remaining == 0:
                    print(
                        f"[{datetime.now().isoformat()}] –ù–µ—Ç –Ω–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–ø–æ–≤—Ç–æ—Ä–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞). –í—ã—Ö–æ–¥."
                    )
                    break
                else:
                    print(
                        f"[{datetime.now().isoformat()}] –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –Ω–∞–π–¥–µ–Ω–æ –µ—â—ë {remaining} –Ω–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤. –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º."
                    )
                    continue

            print(
                f"[{datetime.now().isoformat()}] -> fetched rows: {len(items)}; GPU status check..."
            )

            inputs = []
            metas = []
            for row in items:
                metrics = {
                    "views": row["views"],
                    "forwards": row["forwards"],
                    "reactions_sum": row["reactions_sum"],
                    "comments_count": row["comments_count"],
                    "engagement_rate": row["engagement_rate"],
                }
                inputs.append(
                    {
                        "post_id": row["id"],
                        "channel": row["channel_username"],
                        "text": row["text"],
                        "metrics": metrics,
                    }
                )
                metas.append((row["id"], row["channel_username"]))

            print(
                f"[{datetime.now().isoformat()}] Calling infer_batch for {len(inputs)} items ..."
            )
            judged = infer_batch(inputs)

            upserts = []
            for meta, res, it in zip(metas, judged, inputs):
                pid_item, ch = meta
                signals = {
                    "judge": "llm",
                    "model_version": MODEL_VERSION,
                    "score": res.get("score", 0),
                    "is_good": res.get("is_good", False),
                    "reasons": res.get("reasons", []),
                    "labels": res.get("labels", {}),
                    "metrics": it["metrics"],
                    "raw_output": res.get("raw_output", "")[:2000],
                    "inference_time_s": res.get("inference_time_s", None),
                }

                gen_status = "ok"

                upserts.append(
                    (
                        pid_item,
                        ch,
                        float(signals["score"]),
                        bool(signals["is_good"]),
                        json.dumps(signals, ensure_ascii=False),
                        gen_status,
                    )
                )

            await conn.executemany(UPSERT_SQL, upserts)

            total += len(items)
            print(f"[{datetime.now().isoformat()}]  ‚úì +{len(items)} (–∏—Ç–æ–≥–æ {total})")

    finally:
        await conn.close()


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("Interrupted by user; exiting.")
