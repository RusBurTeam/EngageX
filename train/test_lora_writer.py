# train/test_lora_writer.py
# –¢–µ—Å—Ç –ª–æ–∫–∞–ª—å–Ω–æ–π LoRA-–ø–∏—Å–∞—Ç–µ–ª—è –Ω–∞ Qwen2.5-7B-Instruct

import os
import sys
from datetime import datetime
from typing import List, Dict, Any

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
from dotenv import load_dotenv
import pathlib

# ================== –ë–ê–ó–û–í–´–ï –ü–£–¢–ò ==================
BASE_DIR = str(pathlib.Path(__file__).resolve().parents[1])
if BASE_DIR not in sys.path:
    sys.path.insert(0, BASE_DIR)

load_dotenv(os.path.join(BASE_DIR, ".env"))

# ======== –ü–£–¢–ò –ö –ú–û–î–ï–õ–ò –ò –õ–û–†–ï ========
DEFAULT_LOCAL_QWEN = os.path.join(BASE_DIR, "Models", "qwen2.5-7b-instruct")
BASE_MODEL = os.getenv("QWEN_LOCAL_PATH", DEFAULT_LOCAL_QWEN)

DEFAULT_LORA_DIR = os.path.join(BASE_DIR, "checkpoints", "lora_writer_qwen2_5_7b")
LORA_DIR = os.getenv("LORA_WRITER_OUTPUT", DEFAULT_LORA_DIR)

print(f"[{datetime.now().isoformat()}] üîß TEST CONFIG:")
print(f"  BASE_MODEL = {BASE_MODEL}")
print(f"  LORA_DIR   = {LORA_DIR}")
print("=====================================\n")

if not os.path.isdir(BASE_MODEL):
    raise FileNotFoundError(f"–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –ø–æ –ø—É—Ç–∏: {BASE_MODEL}")

if not os.path.isdir(LORA_DIR):
    raise FileNotFoundError(f"–ü–∞–ø–∫–∞ —Å LoRA-—á–µ–∫–ø–æ–∏–Ω—Ç–æ–º –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {LORA_DIR}")

# ================== –ü–†–û–ú–ü–¢–´, –ö–ê–ö –í –î–ê–¢–ê–°–ï–¢–ï ==================

WRITER_SYSTEM_MSG = (
    "–¢—ã ‚Äî –∞–≤—Ç–æ—Ä –ø–æ—Å—Ç–æ–≤ –¥–ª—è Telegram-–∫–∞–Ω–∞–ª–∞ –ø–æ –∫—Ä–∏–ø—Ç–µ –∏ IT. "
    "–ü–∏—à–µ—à—å —è—Å–Ω–æ, –ø–æ-–¥–µ–ª–æ–≤–æ–º—É, –±–µ–∑ –≤–æ–¥—ã –∏ –∫–ª–∏–∫–±–µ–π—Ç–∞. "
    "–°—Ç–∏–ª—å: –∂–∏–≤–æ–π, –Ω–æ –∞–∫–∫—É—Ä–∞—Ç–Ω—ã–π, –±–µ–∑ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏ –∏ –±–µ–∑ —Ñ–µ–π–∫–æ–≤. "
    "–û–ø–∏—Ä–∞–π—Å—è –Ω–∞ —Ç–µ–º—É –∏ —Ü–µ–ª—å –ø–æ—Å—Ç–∞, —Å–ª–µ–¥–∏ –∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π –∏ –ª–æ–≥–∏–∫–æ–π."
)

WRITER_USER_TEMPLATE = (
    "–ö–∞–Ω–∞–ª: {channel}\n"
    "–¶–µ–ª—å: {goal}\n\n"
    "–§–∞–∫—Ç—É—Ä–∞ (–∫—Ä–∞—Ç–∫–∏–π –±—Ä–∏—Ñ –ø–æ —Ç–µ–º–µ):\n"
    "\"\"\"\n{brief}\n\"\"\"\n\n"
    "–ù–∞–ø–∏—à–∏ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø–æ—Å—Ç –¥–ª—è Telegram-–∫–∞–Ω–∞–ª–∞."
)


def build_messages(channel: str, goal: str, brief: str) -> List[Dict[str, str]]:
    user_content = WRITER_USER_TEMPLATE.format(
        channel=channel or "–Ω–µ —É–∫–∞–∑–∞–Ω",
        goal=goal.strip(),
        brief=brief.strip(),
    )
    return [
        {"role": "system", "content": WRITER_SYSTEM_MSG},
        {"role": "user", "content": user_content},
    ]


# ================== –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò + LORA ==================

def load_lora_model():
    print(f"[{datetime.now().isoformat()}] üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä...")
    tokenizer = AutoTokenizer.from_pretrained(
        BASE_MODEL,
        trust_remote_code=True,
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    print(f"[{datetime.now().isoformat()}] üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å (4bit)...")
    quant_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
    )

    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        trust_remote_code=True,
        quantization_config=quant_config,
        device_map="auto",
    )

    print(f"[{datetime.now().isoformat()}] üîÑ –ù–∞–≤–µ—à–∏–≤–∞–µ–º LoRA –∏–∑ {LORA_DIR}...")
    model = PeftModel.from_pretrained(
        base_model,
        LORA_DIR,
    )
    model.eval()

    try:
        device = model.device
    except Exception:
        params = list(model.parameters())
        device = params[0].device if params else torch.device("cpu")

    print(f"[{datetime.now().isoformat()}] ‚úÖ –ú–æ–¥–µ–ª—å —Å LoRA –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ {device}")
    return tokenizer, model, device


def generate_post(
    tokenizer,
    model,
    device,
    channel: str,
    goal: str,
    brief: str,
    max_new_tokens: int = 256,
) -> str:
    messages = build_messages(channel, goal, brief)

    # –°–æ–±–∏—Ä–∞–µ–º input —á–µ—Ä–µ–∑ chat template Qwen
    prompt_text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,  # –ø—Ä–æ—Å–∏–º –º–æ–¥–µ–ª—å –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å –æ—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    )

    inputs = tokenizer(
        prompt_text,
        return_tensors="pt",
    ).to(device)

    with torch.inference_mode():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )

    gen_ids = output_ids[0][inputs["input_ids"].shape[-1]:]
    gen_text = tokenizer.decode(gen_ids, skip_special_tokens=True)
    return gen_text.strip()


if __name__ == "__main__":
    print(f"[{datetime.now().isoformat()}] üß™ –¢–µ—Å—Ç LoRA-–ø–∏—Å–∞—Ç–µ–ª—è...")

    tokenizer, model, device = load_lora_model()

    # ==== –¢–ï–°–¢–û–í–´–ô –ü–†–ò–ú–ï–† ====
    # ==== –¢–ï–°–¢–û–í–´–ô –ü–†–ò–ú–ï–†: –ß–µ–ª–ª–µ–Ω–¥–∂, –ù–µ–¥–µ–ª—è 1 ‚Äî –í–æ–≤–ª–µ—á–µ–Ω–∏–µ ====
    # ==== –¢–ï–°–¢–û–í–´–ô –ü–†–ò–ú–ï–†: –ø—Ä–æ–¥—É–∫—Ç–æ–≤—ã–π —á–µ–ª–ª–µ–Ω–¥–∂, –ù–µ–¥–µ–ª—è 3 ‚Äî –ü—Ä–æ–¥–∞–∂–∏ / –ö–æ–Ω–≤–µ—Ä—Å–∏—è ====
    test_channel = "taskflow_saas_ru"
    test_goal = (
        "–°–¥–µ–ª–∞—Ç—å –º—è–≥–∫–∏–π –ø—Ä–æ–¥—É–∫—Ç–æ–≤—ã–π —á–µ–ª–ª–µ–Ω–¥–∂ –¥–ª—è –ù–µ–¥–µ–ª–∏ 3 (–ü—Ä–æ–¥–∞–∂–∏ / –ö–æ–Ω–≤–µ—Ä—Å–∏—è): "
        "–ø–æ–¥—Ç–æ–ª–∫–Ω—É—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é '–ï–¥–∏–Ω—ã–π —Ä–∞–±–æ—á–∏–π –¥–µ–Ω—å' –≤ —Å–µ—Ä–≤–∏—Å–µ TaskFlow "
        "–∏ –ø–æ–¥–µ–ª–∏—Ç—å—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º, –Ω–µ —Å–∫–∞—Ç—ã–≤–∞—è—Å—å –≤ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—É—é —Ä–µ–∫–ª–∞–º—É."
    )
    test_brief = (
        "- –ù–µ–¥–µ–ª—è 3: —Ü–µ–ª—å ‚Äî –∫–æ–Ω–≤–µ—Ä—Å–∏—è, –Ω–æ —Ñ–æ—Ä–º–∞—Ç –≤—Å—ë –µ—â—ë –≤—ã–≥–ª—è–¥–∏—Ç –∫–∞–∫ –ø–æ–ª–µ–∑–Ω—ã–π —á–µ–ª–ª–µ–Ω–¥–∂, –∞ –Ω–µ –ª–æ–±–æ–≤–∞—è —Ä–µ–∫–ª–∞–º–∞.\n"
        "- –ü—Ä–æ–¥—É–∫—Ç: TaskFlow ‚Äî SaaS-—Å–µ—Ä–≤–∏—Å –¥–ª—è –ø–æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∑–∞–¥–∞—á –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –¥–Ω—è.\n"
        "- –ö–ª—é—á–µ–≤–∞—è —Ñ–∏—á–∞: —Ä–µ–∂–∏–º '–ï–¥–∏–Ω—ã–π —Ä–∞–±–æ—á–∏–π –¥–µ–Ω—å' ‚Äî –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≤–∏–¥–∏—Ç –≤—Å–µ –∑–∞–¥–∞—á–∏ –Ω–∞ —Å–µ–≥–æ–¥–Ω—è –≤ –æ–¥–Ω–æ–º —Å–ø–∏—Å–∫–µ,\n"
        "  –º–æ–∂–µ—Ç –ø—Ä–æ—Å—Ç–∞–≤–∏—Ç—å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã –∏ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å –≤—Ä–µ–º—è –ø–æ–¥ —Ñ–æ–∫—É—Å.\n"
        "- –ê—É–¥–∏—Ç–æ—Ä–∏—è: —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–¥–ø–∏—Å—á–∏–∫–∏ –∫–∞–Ω–∞–ª–∞ –∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ —É–∂–µ —Å–ª—ã—à–∞–ª–∏ –æ –ø—Ä–æ–¥—É–∫—Ç–µ,\n"
        "  –Ω–æ –º–æ–≥–ª–∏ –µ—â—ë –Ω–µ –ø—Ä–æ–±–æ–≤–∞—Ç—å —ç—Ç—É —Ñ—É–Ω–∫—Ü–∏—é.\n"
        "- –ù—É–∂–Ω–æ: –∫–æ—Ä–æ—Ç–∫–∏–π —á–µ–ª–ª–µ–Ω–¥–∂-–ø–æ—Å—Ç —Å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º –≤–∫–ª—é—á–∏—Ç—å —Ä–µ–∂–∏–º '–ï–¥–∏–Ω—ã–π —Ä–∞–±–æ—á–∏–π –¥–µ–Ω—å' —Å–µ–≥–æ–¥–Ω—è,\n"
        "  –ø–æ—Ä–∞–±–æ—Ç–∞—Ç—å —Å –Ω–∏–º —Ö–æ—Ç—è –±—ã 2‚Äì3 —á–∞—Å–∞ –∏ –∑–∞—Ç–µ–º –Ω–∞–ø–∏—Å–∞—Ç—å –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö, —á—Ç–æ –∏–∑–º–µ–Ω–∏–ª–æ—Å—å –≤ –∏—Ö –¥–Ω–µ.\n"
        "- –¢–æ–Ω: —Å–ø–æ–∫–æ–π–Ω—ã–π, –ø—Ä–æ–¥—É–∫—Ç-—Ü–µ–Ω—Ç—Ä–∏—á–Ω—ã–π, –±–µ–∑ –æ–±–µ—â–∞–Ω–∏–π 'x2 –¥–æ—Ö–æ–¥–∞' –∏ –ø—Ä–æ—á–µ–π –¥–∏—á–∏.\n"
        "- –í –∫–æ–Ω—Ü–µ –¥–æ–ø—É—Å—Ç–∏–º –º—è–≥–∫–∏–π CTA: –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å TaskFlow –ø–æ —Å—Å—ã–ª–∫–µ –∏ —Ä–∞—Å—Å–∫–∞–∑–∞—Ç—å –æ–± –æ–ø—ã—Ç–µ.\n"
        "- –ú–æ–∂–Ω–æ 1‚Äì2 —ç–º–æ–¥–∑–∏, 1‚Äì2 —Ö—ç—à—Ç–µ–≥–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä: #TaskFlow #–ü—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—å."
    )

    print(f"\n[INPUT] –ö–∞–Ω–∞–ª: {test_channel}")
    print(f"[INPUT] –¶–µ–ª—å: {test_goal}")
    print(f"[INPUT] –ë—Ä–∏—Ñ:\n{test_brief}\n")

    out = generate_post(
        tokenizer=tokenizer,
        model=model,
        device=device,
        channel=test_channel,
        goal=test_goal,
        brief=test_brief,
        max_new_tokens=256,
    )

    print("\n[OUTPUT] –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ—Å—Ç LoRA:\n")
    print(out)
    print("\n‚úÖ –¢–µ—Å—Ç –∑–∞–≤–µ—Ä—à—ë–Ω.")
