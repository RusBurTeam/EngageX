# train/test_lora_writer.py
# –¢–µ—Å—Ç –ª–æ–∫–∞–ª—å–Ω–æ–π LoRA-–ø–∏—Å–∞—Ç–µ–ª—è –Ω–∞ Qwen2.5-7B-Instruct

import os
import sys
from datetime import datetime
from typing import List, Dict, Any

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
from dotenv import load_dotenv
import pathlib

# ================== –ë–ê–ó–û–í–´–ï –ü–£–¢–ò ==================
BASE_DIR = str(pathlib.Path(__file__).resolve().parents[1])
if BASE_DIR not in sys.path:
    sys.path.insert(0, BASE_DIR)

load_dotenv(os.path.join(BASE_DIR, ".env"))

# ======== –ü–£–¢–ò –ö –ú–û–î–ï–õ–ò –ò –õ–û–†–ï ========
DEFAULT_LOCAL_QWEN = os.path.join(BASE_DIR, "Models", "qwen2.5-7b-instruct")
BASE_MODEL = os.getenv("QWEN_LOCAL_PATH", DEFAULT_LOCAL_QWEN)

DEFAULT_LORA_DIR = os.path.join(BASE_DIR, "checkpoints", "lora_writer_qwen2_5_7b")
LORA_DIR = os.getenv("LORA_WRITER_OUTPUT", DEFAULT_LORA_DIR)

print(f"[{datetime.now().isoformat()}] üîß TEST CONFIG:")
print(f"  BASE_MODEL = {BASE_MODEL}")
print(f"  LORA_DIR   = {LORA_DIR}")
print("=====================================\n")

if not os.path.isdir(BASE_MODEL):
    raise FileNotFoundError(f"–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –ø–æ –ø—É—Ç–∏: {BASE_MODEL}")

if not os.path.isdir(LORA_DIR):
    raise FileNotFoundError(f"–ü–∞–ø–∫–∞ —Å LoRA-—á–µ–∫–ø–æ–∏–Ω—Ç–æ–º –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {LORA_DIR}")

# ================== –ü–†–û–ú–ü–¢–´, –ö–ê–ö –í –î–ê–¢–ê–°–ï–¢–ï ==================

WRITER_SYSTEM_MSG = (
    "–¢—ã ‚Äî –∞–≤—Ç–æ—Ä –ø–æ—Å—Ç–æ–≤ –¥–ª—è Telegram-–∫–∞–Ω–∞–ª–∞ –ø–æ –∫—Ä–∏–ø—Ç–µ –∏ IT. "
    "–ü–∏—à–µ—à—å —è—Å–Ω–æ, –ø–æ-–¥–µ–ª–æ–≤–æ–º—É, –±–µ–∑ –≤–æ–¥—ã –∏ –∫–ª–∏–∫–±–µ–π—Ç–∞. "
    "–°—Ç–∏–ª—å: –∂–∏–≤–æ–π, –Ω–æ –∞–∫–∫—É—Ä–∞—Ç–Ω—ã–π, –±–µ–∑ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏ –∏ –±–µ–∑ —Ñ–µ–π–∫–æ–≤. "
    "–û–ø–∏—Ä–∞–π—Å—è –Ω–∞ —Ç–µ–º—É –∏ —Ü–µ–ª—å –ø–æ—Å—Ç–∞, —Å–ª–µ–¥–∏ –∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π –∏ –ª–æ–≥–∏–∫–æ–π."
)

WRITER_USER_TEMPLATE = (
    "–ö–∞–Ω–∞–ª: {channel}\n"
    "–¶–µ–ª—å: {goal}\n\n"
    "–§–∞–∫—Ç—É—Ä–∞ (–∫—Ä–∞—Ç–∫–∏–π –±—Ä–∏—Ñ –ø–æ —Ç–µ–º–µ):\n"
    "\"\"\"\n{brief}\n\"\"\"\n\n"
    "–ù–∞–ø–∏—à–∏ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø–æ—Å—Ç –¥–ª—è Telegram-–∫–∞–Ω–∞–ª–∞."
)


def build_messages(channel: str, goal: str, brief: str) -> List[Dict[str, str]]:
    user_content = WRITER_USER_TEMPLATE.format(
        channel=channel or "–Ω–µ —É–∫–∞–∑–∞–Ω",
        goal=goal.strip(),
        brief=brief.strip(),
    )
    return [
        {"role": "system", "content": WRITER_SYSTEM_MSG},
        {"role": "user", "content": user_content},
    ]


# ================== –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò + LORA ==================

def load_lora_model():
    print(f"[{datetime.now().isoformat()}] üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä...")
    tokenizer = AutoTokenizer.from_pretrained(
        BASE_MODEL,
        trust_remote_code=True,
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    print(f"[{datetime.now().isoformat()}] üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å (4bit)...")
    quant_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
    )

    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        trust_remote_code=True,
        quantization_config=quant_config,
        device_map="auto",
    )

    print(f"[{datetime.now().isoformat()}] üîÑ –ù–∞–≤–µ—à–∏–≤–∞–µ–º LoRA –∏–∑ {LORA_DIR}...")
    model = PeftModel.from_pretrained(
        base_model,
        LORA_DIR,
    )
    model.eval()

    try:
        device = model.device
    except Exception:
        params = list(model.parameters())
        device = params[0].device if params else torch.device("cpu")

    print(f"[{datetime.now().isoformat()}] ‚úÖ –ú–æ–¥–µ–ª—å —Å LoRA –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ {device}")
    return tokenizer, model, device


def generate_post(
    tokenizer,
    model,
    device,
    channel: str,
    goal: str,
    brief: str,
    max_new_tokens: int = 256,
) -> str:
    messages = build_messages(channel, goal, brief)

    # –°–æ–±–∏—Ä–∞–µ–º input —á–µ—Ä–µ–∑ chat template Qwen
    prompt_text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,  # –ø—Ä–æ—Å–∏–º –º–æ–¥–µ–ª—å –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å –æ—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    )

    inputs = tokenizer(
        prompt_text,
        return_tensors="pt",
    ).to(device)

    with torch.inference_mode():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )

    gen_ids = output_ids[0][inputs["input_ids"].shape[-1]:]
    gen_text = tokenizer.decode(gen_ids, skip_special_tokens=True)
    return gen_text.strip()


if __name__ == "__main__":
    print(f"[{datetime.now().isoformat()}] üß™ –¢–µ—Å—Ç LoRA-–ø–∏—Å–∞—Ç–µ–ª—è...")

    tokenizer, model, device = load_lora_model()

    # ==== –¢–ï–°–¢–û–í–´–ô –ü–†–ò–ú–ï–† ====
    test_channel = "toncoin_rus"
    test_goal = "–ö—Ä–∞—Ç–∫–æ —Ä–∞—Å—Å–∫–∞–∑–∞—Ç—å –æ –Ω–æ–≤–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ TON –∏ USDT –∏ –ø–æ–∫–∞–∑–∞—Ç—å –ø–æ–ª—å–∑—É –¥–ª—è –æ–±—ã—á–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π."
    test_brief = (
        "- –í –ê—Ñ—Ä–∏–∫–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –ø–æ–ª—É—á–∏–ª–∏ –¥–æ—Å—Ç—É–ø –∫ USDT —á–µ—Ä–µ–∑ TON-–∫–æ—à–µ–ª—ë–∫.\n"
        "- –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –±—ã—Å—Ç—Ä–æ –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ –¥–æ–ª–ª–∞—Ä—ã –±–µ–∑ –±–∞–Ω–∫–æ–≤.\n"
        "- –ö–æ–º–∏—Å—Å–∏–∏ –Ω–∏–∂–µ, –ø–µ—Ä–µ–≤–æ–¥—ã –ø—Ä–æ—Ö–æ–¥—è—Ç –∑–∞ —Å–µ–∫—É–Ω–¥—ã.\n"
        "- –≠—Ç–æ –≤–∞–∂–Ω–æ —Ç–∞–º, –≥–¥–µ –º–µ—Å—Ç–Ω—ã–µ –≤–∞–ª—é—Ç—ã –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã –∏ –µ—Å—Ç—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–æ –¥–æ–ª–ª–∞—Ä—É."
    )

    print(f"\n[INPUT] –ö–∞–Ω–∞–ª: {test_channel}")
    print(f"[INPUT] –¶–µ–ª—å: {test_goal}")
    print(f"[INPUT] –ë—Ä–∏—Ñ:\n{test_brief}\n")

    out = generate_post(
        tokenizer=tokenizer,
        model=model,
        device=device,
        channel=test_channel,
        goal=test_goal,
        brief=test_brief,
        max_new_tokens=256,
    )

    print("\n[OUTPUT] –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ—Å—Ç LoRA:\n")
    print(out)
    print("\n‚úÖ –¢–µ—Å—Ç –∑–∞–≤–µ—Ä—à—ë–Ω.")
