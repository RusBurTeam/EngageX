# train/eval_lora_with_judge.py
# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞: —á–µ—Ä–Ω–æ–≤–∏–∫ vs teacher vs LoRA-–≤—ã—Ö–æ–¥ –ø–æ –º–µ—Ç—Ä–∏–∫–µ judge_quality_llm

import os
import sys
import json
import re
from datetime import datetime
from typing import List, Dict, Any

import torch
from peft import PeftModel
from dotenv import load_dotenv
import pathlib

# --- –±–∞–∑–æ–≤—ã–µ –ø—É—Ç–∏ ---
BASE_DIR = str(pathlib.Path(__file__).resolve().parents[1])
if BASE_DIR not in sys.path:
    sys.path.insert(0, BASE_DIR)

load_dotenv(os.path.join(BASE_DIR, ".env"))

# –ü—É—Ç—å –∫ –≤–∞–ª–∏–¥–Ω–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É:
# 1) WRITER_VAL_PATH –∏–∑ .env
# 2) –¥–µ—Ñ–æ–ª—Ç: data/writer_rewrite_val.jsonl (—Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç —Ç–æ–∂–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º)
VAL_PATH = (
    os.getenv("WRITER_VAL_PATH")
    or os.path.join(BASE_DIR, "data", "writer_rewrite_val.jsonl")
)

# –õ–∏–º–∏—Ç –ø—Ä–∏–º–µ—Ä–æ–≤ –Ω–∞ –ø—Ä–æ–≥–æ–Ω, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 10
VAL_LIMIT = int(os.getenv("WRITER_VAL_LIMIT", "10"))

# –§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø—É —Å—ç–º–ø–ª–æ–≤: all | post | challenge
SAMPLE_TYPE_FILTER = os.getenv("WRITER_SAMPLE_TYPE", "all").strip().lower()
if SAMPLE_TYPE_FILTER not in ("all", "post", "challenge"):
    SAMPLE_TYPE_FILTER = "all"

# –ë–∞–∑–æ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è, –≥–¥–µ –ª–µ–∂–∞—Ç —á–µ–∫–ø–æ–∏–Ω—Ç—ã LoRA-–ø–∏—Å–∞—Ç–µ–ª—è
LORA_BASE_DIR = os.path.join(BASE_DIR, "checkpoints", "lora_writer_qwen2_5_7b")

# –Ω–∞—à loader –±–∞–∑–æ–≤–æ–π Qwen
from Models.qwen_loader import load_tokenizer_model

# judge-–º–æ–¥–µ–ª—å –∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å
from analytics.judge_quality_llm import (
    infer_batch as judge_infer_batch,
    ensure_model as judge_ensure_model,
)


# --------- —É—Ç–∏–ª–∏—Ç–∞: –≤—ã—Ç–∞—â–∏—Ç—å —á–µ—Ä–Ω–æ–≤–∏–∫ –∏–∑ user-–∫–æ–Ω—Ç–µ–Ω—Ç–∞ ---------
def extract_draft_from_user(user_content: str) -> str:
    """
    –°—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç:
      "–í–æ—Ç —á–µ—Ä–Ω–æ–≤–∏–∫ –ø–æ—Å—Ç–∞:\\n\"\"\"\\n...–¢–ï–ö–°–¢...\\n\"\"\"\\n\\n–ü–µ—Ä–µ–ø–∏—à–∏..."
    –ù–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç (–ø–æ—Å—Ç—ã/—á–µ–ª–ª–µ–Ω–¥–∂–∏):
      –ø—Ä–æ—Å—Ç–æ –±—Ä–∏—Ñ –±–µ–∑ —á–µ—Ä–Ω–æ–≤–∏–∫–∞.
    –õ–æ–≥–∏–∫–∞:
      ‚Äì –µ—Å–ª–∏ –µ—Å—Ç—å –±–ª–æ–∫ –º–µ–∂–¥—É \"\"\" ... \"\"\" ‚Äî —Å—á–∏—Ç–∞–µ–º —ç—Ç–æ —á–µ—Ä–Ω–æ–≤–∏–∫–æ–º;
      ‚Äì –∏–Ω–∞—á–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≤–µ—Å—å user_content.
    """
    m = re.search(r'"""(.*?)"""', user_content, flags=re.S)
    if m:
        draft = m.group(1).strip()
        if draft:
            return draft
    return user_content.strip()


# --------- –∑–∞–≥—Ä—É–∑–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ ---------
def load_val_examples(limit: int) -> List[Dict[str, Any]]:
    if not os.path.exists(VAL_PATH):
        raise FileNotFoundError(f"–ù–µ –Ω–∞–π–¥–µ–Ω val-–¥–∞—Ç–∞—Å–µ—Ç: {VAL_PATH}")

    examples: List[Dict[str, Any]] = []
    total_lines = 0
    taken_lines = 0

    print(f"[{datetime.now().isoformat()}] üìÇ –ß–∏—Ç–∞–µ–º val-–¥–∞—Ç–∞—Å–µ—Ç: {VAL_PATH}")
    print(f"[{datetime.now().isoformat()}] üîé SAMPLE_TYPE_FILTER = {SAMPLE_TYPE_FILTER}")
    print(f"[{datetime.now().isoformat()}] üîé VAL_LIMIT = {limit}")

    with open(VAL_PATH, "r", encoding="utf-8") as f:
        for line in f:
            total_lines += 1
            line = line.strip()
            if not line:
                continue
            obj = json.loads(line)

            # –ï—Å–ª–∏ –µ—Å—Ç—å –ø–æ–ª–µ sample_type –∏ –≤–∫–ª—é—á—ë–Ω —Ñ–∏–ª—å—Ç—Ä
            if SAMPLE_TYPE_FILTER != "all" and "sample_type" in obj:
                st = str(obj.get("sample_type", "")).lower()
                if st != SAMPLE_TYPE_FILTER:
                    continue

            msgs = obj.get("messages", [])
            if len(msgs) < 3:
                # –æ–∂–∏–¥–∞–µ–º system, user, assistant
                continue

            system_msg = msgs[0].get("content", "")
            user_msg = msgs[1].get("content", "")
            assistant = msgs[2].get("content", "")

            draft = extract_draft_from_user(user_msg)

            examples.append(
                {
                    "system": system_msg,
                    "user": user_msg,
                    "draft": draft,
                    "ref": assistant,
                    "sample_type": obj.get("sample_type", None),
                }
            )
            taken_lines += 1

            if len(examples) >= limit:
                break

    print(
        f"[{datetime.now().isoformat()}] –í–∑—è—Ç–æ {len(examples)} –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ val "
        f"(–ø—Ä–æ—á–∏—Ç–∞–Ω–æ —Å—Ç—Ä–æ–∫: {total_lines}, –ø—Ä–æ—à–ª–æ —Ñ–∏–ª—å—Ç—Ä: {taken_lines})."
    )
    return examples


# --------- –ø–æ–∏—Å–∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å LoRA-–∞–¥–∞–ø—Ç–µ—Ä–∞–º–∏ ---------
def resolve_lora_dir() -> str:
    """
    –ù–∞—Ö–æ–¥–∏–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –≥–¥–µ —Ä–µ–∞–ª—å–Ω–æ –ª–µ–∂–∏—Ç adapter_config.json LoRA-–ø–∏—Å–∞—Ç–µ–ª—è.

    –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:
    1) –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è LORA_WRITER (–µ—Å–ª–∏ –ø—É—Ç—å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ —Ç–∞–º –µ—Å—Ç—å adapter_config.json).
    2) –ü—Ä—è–º–æ –≤ LORA_BASE_DIR.
    3) –õ—é–±–∞—è –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è LORA_BASE_DIR/*, –≥–¥–µ –µ—Å—Ç—å adapter_config.json.
       –ë–µ—Ä—ë–º —Å–∞–º—É—é "–ø–æ–∑–¥–Ω—é—é" –ø–æ –∏–º–µ–Ω–∏ (—á–∞—Å—Ç–æ —ç—Ç–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π checkpoint).
    """
    # 1) .env / –æ–∫—Ä—É–∂–µ–Ω–∏–µ
    env_dir = os.getenv("LORA_WRITER")
    if env_dir:
        env_dir = os.path.abspath(env_dir)
        cfg = os.path.join(env_dir, "adapter_config.json")
        if os.path.exists(cfg):
            print(
                f"[{datetime.now().isoformat()}] üìå –ò—Å–ø–æ–ª—å–∑—É–µ–º LORA_WRITER –∏–∑ .env: {env_dir}"
            )
            return env_dir
        else:
            print(
                f"[{datetime.now().isoformat()}] ‚ö†Ô∏è –í LORA_WRITER –Ω–µ—Ç adapter_config.json: {cfg}"
            )

    # 2) –ü—Ä—è–º–æ –≤ LORA_BASE_DIR
    base_cfg = os.path.join(LORA_BASE_DIR, "adapter_config.json")
    if os.path.exists(base_cfg):
        print(
            f"[{datetime.now().isoformat()}] üìå –ù–∞–π–¥–µ–Ω adapter_config.json –≤ {LORA_BASE_DIR}"
        )
        return LORA_BASE_DIR

    # 3) –ü–æ–∏—Å–∫ –≤ –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è—Ö (checkpoint-1, checkpoint-12 –∏ —Ç.–ø.)
    candidates = []
    if os.path.isdir(LORA_BASE_DIR):
        for name in sorted(os.listdir(LORA_BASE_DIR)):
            subdir = os.path.join(LORA_BASE_DIR, name)
            if not os.path.isdir(subdir):
                continue
            cfg = os.path.join(subdir, "adapter_config.json")
            if os.path.exists(cfg):
                candidates.append(subdir)

    if candidates:
        chosen = candidates[-1]
        print(
            f"[{datetime.now().isoformat()}] üìå –ù–∞–π–¥–µ–Ω–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ LoRA-—á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º: {chosen}"
        )
        return chosen

    msg_lines = [
        "–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ LoRA-–∞–¥–∞–ø—Ç–µ—Ä (adapter_config.json).",
        f"–ü—Ä–æ–≤–µ—Ä–µ–Ω—ã –ø—É—Ç–∏:",
        f"  ‚Ä¢ LORA_WRITER={env_dir or '–Ω–µ –∑–∞–¥–∞–Ω–∞'}",
        f"  ‚Ä¢ {LORA_BASE_DIR} –∏ –µ–≥–æ –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏",
        "–£–±–µ–¥–∏—Å—å, —á—Ç–æ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è LoRA —É —Ç–µ–±—è –µ—Å—Ç—å –ø–∞–ø–∫–∞ —Å —Ñ–∞–π–ª–∞–º–∏ adapter_config.json –∏ adapter_model.*",
    ]
    raise FileNotFoundError("\n".join(msg_lines))


# --------- –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å LoRA-–ø–∏—Å–∞—Ç–µ–ª–µ–º ---------
def load_writer_lora():
    print(f"[{datetime.now().isoformat()}] üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å + LoRA-Writer...")
    tokenizer, base_model = load_tokenizer_model()
    base_model.eval()
    device = base_model.device if hasattr(base_model, "device") else torch.device("cpu")

    lora_dir = resolve_lora_dir()
    print(f"[{datetime.now().isoformat()}] üîó LORA_DIR = {lora_dir}")

    lora_model = PeftModel.from_pretrained(
        base_model,
        lora_dir,
        torch_dtype=base_model.dtype,
        is_trainable=False,
    )
    lora_model.eval()
    print(f"[{datetime.now().isoformat()}] ‚úÖ LoRA –ø–æ–¥–∫–ª—é—á–µ–Ω–∞. device = {device}")
    return tokenizer, lora_model, device


def generate_with_lora(
    tokenizer,
    model,
    device,
    system_text: str,
    user_text: str,
    max_new_tokens: int = 512,
) -> str:
    messages = [
        {"role": "system", "content": system_text},
        {"role": "user", "content": user_text},
    ]
    try:
        inb = tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        )
    except TypeError:
        inb = tokenizer.apply_chat_template(messages, return_tensors="pt")

    if isinstance(inb, torch.Tensor):
        input_ids = inb.to(device)
        attention_mask = torch.ones_like(input_ids, dtype=torch.long, device=device)
    elif isinstance(inb, dict):
        input_ids = inb["input_ids"].to(device)
        attention_mask = inb.get(
            "attention_mask",
            torch.ones_like(inb["input_ids"], dtype=torch.long),
        ).to(device)
    else:
        raise RuntimeError(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏: {type(inb)}")

    with torch.inference_mode():
        out = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_new_tokens=max_new_tokens,
            do_sample=False,
            pad_token_id=getattr(tokenizer, "eos_token_id", None),
            eos_token_id=getattr(tokenizer, "eos_token_id", None),
        )

    seq_len = input_ids.shape[1]
    gen_ids = out[0][seq_len:]
    text = tokenizer.decode(gen_ids, skip_special_tokens=True)
    return text.strip()


# --------- –æ—Ü–µ–Ω–∫–∞ —á–µ—Ä–µ–∑ judge_quality_llm ---------
def evaluate_with_judge(drafts: List[str], refs: List[str], loras: List[str]):
    """
    –ü—Ä–æ–≥–æ–Ω—è–µ–º –≤—Å–µ —Ç—Ä–∏ –≥—Ä—É–ø–ø—ã —á–µ—Ä–µ–∑ judge_quality_llm –∏ —Å—á–∏—Ç–∞–µ–º —Å—Ä–µ–¥–Ω–∏–µ score.
    –ü–ª—é—Å –≤—ã–≤–æ–¥–∏–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ (teacher vs LoRA).
    """
    judge_ensure_model()  # –≥—Ä—É–∑–∏–º –º–æ–¥–µ–ª—å-—Å—É–¥—å—é

    items = []
    kind_idx = []  # —á—Ç–æ–±—ã –ø–æ–º–Ω–∏—Ç—å, –∫–∞–∫–æ–π —Ç–µ–∫—Å—Ç –∫–∞–∫–æ–≥–æ —Ç–∏–ø–∞

    # 0 = draft, 1 = ref, 2 = lora
    pid = 1
    for d in drafts:
        items.append(
            {
                "post_id": pid,
                "channel": "eval-draft",
                "text": d,
                "metrics": {
                    "views": 0,
                    "forwards": 0,
                    "reactions_sum": 0,
                    "comments_count": 0,
                    "engagement_rate": 0.0,
                },
            }
        )
        kind_idx.append(0)
        pid += 1
    for r in refs:
        items.append(
            {
                "post_id": pid,
                "channel": "eval-ref",
                "text": r,
                "metrics": {
                    "views": 0,
                    "forwards": 0,
                    "reactions_sum": 0,
                    "comments_count": 0,
                    "engagement_rate": 0.0,
                },
            }
        )
        kind_idx.append(1)
        pid += 1
    for l in loras:
        items.append(
            {
                "post_id": pid,
                "channel": "eval-lora",
                "text": l,
                "metrics": {
                    "views": 0,
                    "forwards": 0,
                    "reactions_sum": 0,
                    "comments_count": 0,
                    "engagement_rate": 0.0,
                },
            }
        )
        kind_idx.append(2)
        pid += 1

    print(
        f"[{datetime.now().isoformat()}] ‚öñÔ∏è –û—Ç–ø—Ä–∞–≤–ª—è–µ–º {len(items)} —Ç–µ–∫—Å—Ç–æ–≤ –≤ judge_quality_llm..."
    )
    results = judge_infer_batch(items)

    n = len(drafts)
    if len(results) != 3 * n:
        print(
            f"[{datetime.now().isoformat()}] ‚ö†Ô∏è –û–∂–∏–¥–∞–ª–æ—Å—å {3*n} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ç judge, –ø–æ–ª—É—á–µ–Ω–æ {len(results)}."
        )

    # —Ä–∞–∑–±–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ç—Ä–∏ –±–ª–æ–∫–∞
    res_draft = results[0:n]
    res_ref = results[n : 2 * n]
    res_lora = results[2 * n : 3 * n]

    # —Å–æ–±–∏—Ä–∞–µ–º –ø–æ —Ç–∏–ø–∞–º (–¥–ª—è —Å—Ä–µ–¥–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π)
    sums = {0: 0.0, 1: 0.0, 2: 0.0}
    counts = {0: n, 1: n, 2: n}

    for r in res_draft:
        sums[0] += float(r.get("score", 0.0))
    for r in res_ref:
        sums[1] += float(r.get("score", 0.0))
    for r in res_lora:
        sums[2] += float(r.get("score", 0.0))

    avg = {
        k: (sums[k] / counts[k] if counts[k] > 0 else 0.0)
        for k in sums.keys()
    }

    # –∫—Ç–æ –≤—ã–∏–≥—Ä–∞–ª
    label_map = {0: "draft", 1: "teacher", 2: "lora"}
    best_k = max(avg, key=avg.get)
    best_label = label_map[best_k]

    print("\n========== üìä –û–¶–ï–ù–ö–ê –ß–ï–†–ï–ó JUDGE ==========")
    print(f"–ß–µ—Ä–Ω–æ–≤–∏–∫–∏ (draft):   n={counts[0]}  avg_score={avg[0]:.2f}")
    print(f"Teacher (—Ä–µ—Ñ–µ—Ä–µ–Ω—Å—ã): n={counts[1]}  avg_score={avg[1]:.2f}")
    print(f"LoRA-–≤—ã—Ö–æ–¥:          n={counts[2]}  avg_score={avg[2]:.2f}")
    print("-------------------------------------------")
    print(f"ü•á –õ—É—á—à–∏–π –ø–æ —Å—Ä–µ–¥–Ω–µ–π –æ—Ü–µ–Ω–∫–µ: {best_label}")
    print(
        f"Œî(LoRA - teacher) = {avg[2] - avg[1]:+.2f}   |   Œî(LoRA - draft) = {avg[2] - avg[0]:+.2f}"
    )
    print("===========================================\n")

    # ------ –ü—Ä–∏–º–µ—Ä—ã –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ (teacher vs LoRA) ------
    max_examples = min(3, n)
    if max_examples > 0:
        print("------ –ü—Ä–∏–º–µ—Ä—ã –≤—ã–≤–æ–¥–∞ (teacher vs LoRA) ------")
    for i in range(max_examples):
        s_draft = float(res_draft[i].get("score", 0.0))
        s_ref = float(res_ref[i].get("score", 0.0))
        s_lora = float(res_lora[i].get("score", 0.0))

        print(f"\n=== –ü—Ä–∏–º–µ—Ä {i+1} ===")
        print(
            f"–û—Ü–µ–Ω–∫–∏ judge: draft={s_draft:.1f} | teacher={s_ref:.1f} | lora={s_lora:.1f}"
        )

        ref_text = refs[i].strip()
        lora_text = loras[i].strip()

        # —á—Ç–æ–±—ã –Ω–µ —Å–ø–∞–º–∏—Ç—å –∫–æ–Ω—Å–æ–ª—å ‚Äî —Ä–µ–∂–µ–º –ø–æ ~600 —Å–∏–º–≤–æ–ª–æ–≤
        def cut(t: str, max_len: int = 600) -> str:
            return t if len(t) <= max_len else t[:max_len] + "...\n[–æ–±—Ä–µ–∑–∞–Ω–æ]"

        print("\n--- Teacher (—Ä–µ—Ñ–µ—Ä–µ–Ω—Å) ---")
        print(cut(ref_text))

        print("\n--- LoRA OUTPUT ---")
        print(cut(lora_text))

    if max_examples > 0:
        print("\n------ –ö–æ–Ω–µ—Ü –ø—Ä–∏–º–µ—Ä–æ–≤ ------\n")


def main():
    print(f"[{datetime.now().isoformat()}] üîç –¢–µ—Å—Ç–∏—Ä—É–µ–º LoRA-Writer –Ω–∞ val + judge_quality_llm")
    print(f"[{datetime.now().isoformat()}] VAL_PATH = {VAL_PATH}")

    examples = load_val_examples(limit=VAL_LIMIT)
    if not examples:
        print("‚ùå –í val-–¥–∞—Ç–∞—Å–µ—Ç–µ –Ω–µ—Ç –ø—Ä–∏–º–µ—Ä–æ–≤. –ü—Ä–æ–≤–µ—Ä—å WRITER_VAL_PATH / —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞.")
        return

    tokenizer, lora_model, device = load_writer_lora()

    drafts: List[str] = []
    refs: List[str] = []
    loras: List[str] = []

    total = len(examples)
    print(f"[{datetime.now().isoformat()}] üîÑ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç—ã LoRA –¥–ª—è {total} –ø—Ä–∏–º–µ—Ä–æ–≤...")

    for idx, ex in enumerate(examples, start=1):
        system_text = ex["system"]
        user_text = ex["user"]
        draft = ex["draft"]
        ref = ex["ref"]

        lora_out = generate_with_lora(
            tokenizer,
            lora_model,
            device,
            system_text,
            user_text,
        )

        drafts.append(draft)
        refs.append(ref)
        loras.append(lora_out)

        if idx % 5 == 0 or idx == total:
            print(
                f"[{datetime.now().isoformat()}] –ü—Ä–æ–≥—Ä–µ—Å—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ LoRA: {idx}/{total}"
            )

    evaluate_with_judge(drafts, refs, loras)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("–û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.")
