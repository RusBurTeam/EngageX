# train/eval_lora_with_judge.py
# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞: —á–µ—Ä–Ω–æ–≤–∏–∫ vs teacher vs LoRA-–≤—ã—Ö–æ–¥ –ø–æ –º–µ—Ç—Ä–∏–∫–µ judge_quality_llm

import os
import sys
import json
import re
from datetime import datetime
from typing import List, Dict, Any

import torch
from peft import PeftModel
from dotenv import load_dotenv
import pathlib

# --- –±–∞–∑–æ–≤—ã–µ –ø—É—Ç–∏ ---
BASE_DIR = str(pathlib.Path(__file__).resolve().parents[1])
if BASE_DIR not in sys.path:
    sys.path.insert(0, BASE_DIR)

load_dotenv(os.path.join(BASE_DIR, ".env"))

VAL_PATH = os.path.join(BASE_DIR, "data", "writer_rewrite_val.jsonl")
# –ë–∞–∑–æ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è, –≥–¥–µ –ª–µ–∂–∞—Ç —á–µ–∫–ø–æ–∏–Ω—Ç—ã LoRA-–ø–∏—Å–∞—Ç–µ–ª—è
LORA_BASE_DIR = os.path.join(BASE_DIR, "checkpoints", "lora_writer_qwen2_5_7b")

# –Ω–∞—à loader –±–∞–∑–æ–≤–æ–π Qwen
from Models.qwen_loader import load_tokenizer_model

# judge-–º–æ–¥–µ–ª—å –∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å
from analytics.judge_quality_llm import infer_batch as judge_infer_batch, ensure_model as judge_ensure_model


# --------- —É—Ç–∏–ª–∏—Ç–∞: –≤—ã—Ç–∞—â–∏—Ç—å —á–µ—Ä–Ω–æ–≤–∏–∫ –∏–∑ user-–∫–æ–Ω—Ç–µ–Ω—Ç–∞ ---------
def extract_draft_from_user(user_content: str) -> str:
    """
    –í –¥–∞—Ç–∞—Å–µ—Ç–µ user, –∫–∞–∫ –ø—Ä–∞–≤–∏–ª–æ, –∏–º–µ–µ—Ç —Ñ–æ—Ä–º–∞—Ç:
    "–í–æ—Ç —á–µ—Ä–Ω–æ–≤–∏–∫ –ø–æ—Å—Ç–∞:\n\"\"\"\n...–¢–ï–ö–°–¢...\n\"\"\"\n\n–ü–µ—Ä–µ–ø–∏—à–∏..."
    –ü–æ–ø—Ä–æ–±—É–µ–º –≤—ã—Ç–∞—â–∏—Ç—å —Ç–µ–∫—Å—Ç –º–µ–∂–¥—É –±–ª–æ–∫–∞–º–∏ —Å —Ç—Ä–æ–π–Ω—ã–º–∏ –∫–∞–≤—ã—á–∫–∞–º–∏ (\"\"\" ... \"\"\").
    –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å ‚Äî –≤–µ—Ä–Ω—ë–º –ø–æ–ª–Ω—ã–π user_content.
    """
    m = re.search(r'"""(.*?)"""', user_content, flags=re.S)
    if m:
        draft = m.group(1).strip()
        if draft:
            return draft
    return user_content.strip()


# --------- –∑–∞–≥—Ä—É–∑–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ ---------
def load_val_examples(limit: int = 10) -> List[Dict[str, Any]]:
    if not os.path.exists(VAL_PATH):
        raise FileNotFoundError(f"–ù–µ –Ω–∞–π–¥–µ–Ω val-–¥–∞—Ç–∞—Å–µ—Ç: {VAL_PATH}")

    examples = []
    with open(VAL_PATH, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            obj = json.loads(line)
            msgs = obj.get("messages", [])
            if len(msgs) < 3:
                continue
            system_msg = msgs[0]["content"]
            user_msg   = msgs[1]["content"]
            assistant  = msgs[2]["content"]
            draft = extract_draft_from_user(user_msg)
            examples.append({
                "system": system_msg,
                "user": user_msg,
                "draft": draft,
                "ref": assistant
            })
            if len(examples) >= limit:
                break

    print(f"[{datetime.now().isoformat()}] –í–∑—è—Ç–æ {len(examples)} –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ val.")
    return examples


# --------- –ø–æ–∏—Å–∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å LoRA-–∞–¥–∞–ø—Ç–µ—Ä–∞–º–∏ ---------
def resolve_lora_dir() -> str:
    """
    –ù–∞—Ö–æ–¥–∏–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –≥–¥–µ —Ä–µ–∞–ª—å–Ω–æ –ª–µ–∂–∏—Ç adapter_config.json LoRA-–ø–∏—Å–∞—Ç–µ–ª—è.

    –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:
    1) –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è LORA_WRITER_DIR (–µ—Å–ª–∏ –ø—É—Ç—å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ —Ç–∞–º –µ—Å—Ç—å adapter_config.json).
    2) –ü—Ä—è–º–æ –≤ LORA_BASE_DIR.
    3) –õ—é–±–∞—è –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è LORA_BASE_DIR/*, –≥–¥–µ –µ—Å—Ç—å adapter_config.json.
       –ë–µ—Ä—ë–º —Å–∞–º—É—é "–ø–æ–∑–¥–Ω—é—é" –ø–æ –∏–º–µ–Ω–∏ (—á–∞—Å—Ç–æ —ç—Ç–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π checkpoint).
    """
    # 1) .env / –æ–∫—Ä—É–∂–µ–Ω–∏–µ
    env_dir = os.getenv("LORA_WRITER")
    if env_dir:
        env_dir = os.path.abspath(env_dir)
        cfg = os.path.join(env_dir, "adapter_config.json")
        if os.path.exists(cfg):
            print(f"[{datetime.now().isoformat()}] üìå –ò—Å–ø–æ–ª—å–∑—É–µ–º LORA_WRITER –∏–∑ .env: {env_dir}")
            return env_dir
        else:
            print(f"[{datetime.now().isoformat()}] ‚ö†Ô∏è –í LORA_WRITER_DIR –Ω–µ—Ç adapter_config.json: {cfg}")

    # 2) –ü—Ä—è–º–æ –≤ LORA_BASE_DIR
    base_cfg = os.path.join(LORA_BASE_DIR, "adapter_config.json")
    if os.path.exists(base_cfg):
        print(f"[{datetime.now().isoformat()}] üìå –ù–∞–π–¥–µ–Ω adapter_config.json –≤ {LORA_BASE_DIR}")
        return LORA_BASE_DIR

    # 3) –ü–æ–∏—Å–∫ –≤ –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è—Ö (checkpoint-1, checkpoint-12 –∏ —Ç.–ø.)
    candidates = []
    if os.path.isdir(LORA_BASE_DIR):
        for name in sorted(os.listdir(LORA_BASE_DIR)):
            subdir = os.path.join(LORA_BASE_DIR, name)
            if not os.path.isdir(subdir):
                continue
            cfg = os.path.join(subdir, "adapter_config.json")
            if os.path.exists(cfg):
                candidates.append(subdir)

    if candidates:
        # –ë–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–æ –∏–º–µ–Ω–∏ (—á–∞—Å—Ç–æ —ç—Ç–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç)
        chosen = candidates[-1]
        print(f"[{datetime.now().isoformat()}] üìå –ù–∞–π–¥–µ–Ω–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ LoRA-—á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º: {chosen}")
        return chosen

    # –ï—Å–ª–∏ –≤–æ–æ–±—â–µ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏ ‚Äî –¥–∞—ë–º —á–µ—Å—Ç–Ω—É—é –æ—à–∏–±–∫—É —Å –ø–æ–¥—Å–∫–∞–∑–∫–∞–º–∏
    msg_lines = [
        "–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ LoRA-–∞–¥–∞–ø—Ç–µ—Ä (adapter_config.json).",
        f"–ü—Ä–æ–≤–µ—Ä–µ–Ω—ã –ø—É—Ç–∏:",
        f"  ‚Ä¢ LORA_WRITER_DIR={env_dir or '–Ω–µ –∑–∞–¥–∞–Ω–∞'}",
        f"  ‚Ä¢ {LORA_BASE_DIR} –∏ –µ–≥–æ –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏",
        "–£–±–µ–¥–∏—Å—å, —á—Ç–æ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è LoRA —É —Ç–µ–±—è –µ—Å—Ç—å –ø–∞–ø–∫–∞ —Å —Ñ–∞–π–ª–∞–º–∏ adapter_config.json –∏ adapter_model.*",
    ]
    raise FileNotFoundError("\n".join(msg_lines))


# --------- –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å LoRA-–ø–∏—Å–∞—Ç–µ–ª–µ–º ---------
def load_writer_lora():
    print(f"[{datetime.now().isoformat()}] üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å + LoRA-Writer...")
    tokenizer, base_model = load_tokenizer_model()
    base_model.eval()
    device = base_model.device if hasattr(base_model, "device") else torch.device("cpu")

    lora_dir = resolve_lora_dir()
    print(f"[{datetime.now().isoformat()}] üîó LORA_DIR = {lora_dir}")

    lora_model = PeftModel.from_pretrained(
        base_model,
        lora_dir,
        torch_dtype=base_model.dtype,
        is_trainable=False,
    )
    lora_model.eval()
    print(f"[{datetime.now().isoformat()}] ‚úÖ LoRA –ø–æ–¥–∫–ª—é—á–µ–Ω–∞. device = {device}")
    return tokenizer, lora_model, device


def generate_with_lora(tokenizer, model, device, system_text: str, user_text: str, max_new_tokens: int = 512) -> str:
    messages = [
        {"role": "system", "content": system_text},
        {"role": "user", "content": user_text},
    ]
    try:
        inb = tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_tensors="pt"
        )
    except TypeError:
        inb = tokenizer.apply_chat_template(
            messages,
            return_tensors="pt"
        )

    if isinstance(inb, torch.Tensor):
        input_ids = inb.to(device)
        attention_mask = torch.ones_like(input_ids, dtype=torch.long, device=device)
    elif isinstance(inb, dict):
        input_ids = inb["input_ids"].to(device)
        attention_mask = inb.get(
            "attention_mask",
            torch.ones_like(inb["input_ids"], dtype=torch.long)
        ).to(device)
    else:
        raise RuntimeError(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏: {type(inb)}")

    with torch.inference_mode():
        out = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_new_tokens=max_new_tokens,
            do_sample=False,
            pad_token_id=getattr(tokenizer, "eos_token_id", None),
            eos_token_id=getattr(tokenizer, "eos_token_id", None),
        )

    seq_len = input_ids.shape[1]
    gen_ids = out[0][seq_len:]
    text = tokenizer.decode(gen_ids, skip_special_tokens=True)
    return text.strip()


# --------- –æ—Ü–µ–Ω–∫–∞ —á–µ—Ä–µ–∑ judge_quality_llm ---------
def evaluate_with_judge(drafts: List[str], refs: List[str], loras: List[str]):
    """
    –ü—Ä–æ–≥–æ–Ω—è–µ–º –≤—Å–µ —Ç—Ä–∏ –≥—Ä—É–ø–ø—ã —á–µ—Ä–µ–∑ judge_quality_llm –∏ —Å—á–∏—Ç–∞–µ–º —Å—Ä–µ–¥–Ω–∏–µ score.
    """
    judge_ensure_model()  # –≥—Ä—É–∑–∏–º –º–æ–¥–µ–ª—å-—Å—É–¥—å—é

    items = []
    kind_idx = []  # —á—Ç–æ–±—ã –ø–æ–º–Ω–∏—Ç—å, –∫–∞–∫–æ–π —Ç–µ–∫—Å—Ç –∫–∞–∫–æ–≥–æ —Ç–∏–ø–∞

    # 0 = draft, 1 = ref, 2 = lora
    pid = 1
    for d in drafts:
        items.append({
            "post_id": pid,
            "channel": "eval-draft",
            "text": d,
            "metrics": {"views": 0, "forwards": 0, "reactions_sum": 0, "comments_count": 0, "engagement_rate": 0.0}
        })
        kind_idx.append(0)
        pid += 1
    for r in refs:
        items.append({
            "post_id": pid,
            "channel": "eval-ref",
            "text": r,
            "metrics": {"views": 0, "forwards": 0, "reactions_sum": 0, "comments_count": 0, "engagement_rate": 0.0}
        })
        kind_idx.append(1)
        pid += 1
    for l in loras:
        items.append({
            "post_id": pid,
            "channel": "eval-lora",
            "text": l,
            "metrics": {"views": 0, "forwards": 0, "reactions_sum": 0, "comments_count": 0, "engagement_rate": 0.0}
        })
        kind_idx.append(2)
        pid += 1

    print(f"[{datetime.now().isoformat()}] ‚öñÔ∏è –û—Ç–ø—Ä–∞–≤–ª—è–µ–º {len(items)} —Ç–µ–∫—Å—Ç–æ–≤ –≤ judge_quality_llm...")
    results = judge_infer_batch(items)

    # —Å–æ–±–∏—Ä–∞–µ–º –ø–æ —Ç–∏–ø–∞–º
    sums = {0: 0.0, 1: 0.0, 2: 0.0}
    counts = {0: 0, 1: 0, 2: 0}

    for k, res in zip(kind_idx, results):
        score = float(res.get("score", 0.0))
        sums[k] += score
        counts[k] += 1

    avg = {k: (sums[k] / counts[k] if counts[k] > 0 else 0.0) for k in sums.keys()}

    print("\n========== üìä –û–¶–ï–ù–ö–ê –ß–ï–†–ï–ó JUDGE ==========")
    print(f"–ß–µ—Ä–Ω–æ–≤–∏–∫–∏ (draft):   n={counts[0]}  avg_score={avg[0]:.2f}")
    print(f"Teacher (—Ä–µ—Ñ–µ—Ä–µ–Ω—Å—ã): n={counts[1]}  avg_score={avg[1]:.2f}")
    print(f"LoRA-–≤—ã—Ö–æ–¥:          n={counts[2]}  avg_score={avg[2]:.2f}")
    print("===========================================\n")


def main():
    print(f"[{datetime.now().isoformat()}] üîç –¢–µ—Å—Ç–∏—Ä—É–µ–º LoRA-Writer –Ω–∞ val + judge_quality_llm")

    examples = load_val_examples(limit=10)
    if not examples:
        print("‚ùå –í val-–¥–∞—Ç–∞—Å–µ—Ç–µ –Ω–µ—Ç –ø—Ä–∏–º–µ—Ä–æ–≤. –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏ split_writer_dataset.py –∏ –ø—Ä–æ–≤–µ—Ä—å —Ñ–∞–π–ª—ã.")
        return

    tokenizer, lora_model, device = load_writer_lora()

    drafts = []
    refs = []
    loras = []

    for i, ex in enumerate(examples, start=1):
        system_text = ex["system"]
        user_text   = ex["user"]
        draft = ex["draft"]
        ref   = ex["ref"]

        print(f"\n----- –ü—Ä–∏–º–µ—Ä {i} -----")
        print("DRAFT:")
        print(draft)
        print("\nREF (teacher):")
        print(ref)
        lora_out = generate_with_lora(tokenizer, lora_model, device, system_text, user_text)
        print("\nLoRA OUTPUT:")
        print(lora_out)

        drafts.append(draft)
        refs.append(ref)
        loras.append(lora_out)

    # –û—Ü–µ–Ω–∫–∞ —á–µ—Ä–µ–∑ judge
    evaluate_with_judge(drafts, refs, loras)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("–û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.")
