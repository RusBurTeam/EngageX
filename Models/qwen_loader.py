# Models/qwen_loader.py
#
# –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ª–æ–∞–¥–µ—Ä Qwen2.5-7B-Instruct:
# - —Å–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ—Ç—Å—è –≤–∑—è—Ç—å –ª–æ–∫–∞–ª—å–Ω—É—é –ø–∞–ø–∫—É –º–æ–¥–µ–ª–∏ –≤ –ø—Ä–æ–µ–∫—Ç–µ
# - –ø—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏ –º–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —á–µ—Ä–µ–∑ .env (BASE_MODEL=...)
# - –≥—Ä—É–∑–∏—Ç –≤ 4-–±–∏—Ç–Ω–æ–º —Ä–µ–∂–∏–º–µ —á–µ—Ä–µ–∑ BitsAndBytes (–æ–ø—Ç–∏–º–∞–ª—å–Ω–æ –ø–æ–¥ LoRA)
# - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç (tokenizer, model), –∫–∞–∫ –∂–¥—É—Ç judge_quality_llm –∏ train_lora_writer

from __future__ import annotations

import os
import pathlib
from typing import Tuple

import torch
from dotenv import load_dotenv
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
)

try:
    from transformers import BitsAndBytesConfig
except Exception:
    BitsAndBytesConfig = None  # –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π, –Ω–æ –¥–ª—è 4bit –ª—É—á—à–µ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å bitsandbytes

# ----------------------------------------------------
# –ë–∞–∑–æ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –ø—Ä–æ–µ–∫—Ç–∞ –∏ .env
# ----------------------------------------------------

BASE_DIR = pathlib.Path(__file__).resolve().parents[1]
ENV_PATH = BASE_DIR / ".env"
if ENV_PATH.exists():
    load_dotenv(str(ENV_PATH))


def _resolve_model_name() -> str:
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –æ—Ç–∫—É–¥–∞ –±—Ä–∞—Ç—å –º–æ–¥–µ–ª—å:

    1) –ï—Å–ª–∏ –≤ .env –∑–∞–¥–∞–Ω BASE_MODEL ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –∫–∞–∫ –ø—É—Ç—å/–Ω–∞–∑–≤–∞–Ω–∏–µ.
    2) –ò–Ω–∞—á–µ, –µ—Å–ª–∏ –≤ –ø—Ä–æ–µ–∫—Ç–µ –µ—Å—Ç—å –ª–æ–∫–∞–ª—å–Ω–∞—è –ø–∞–ø–∫–∞ qwen2.5-7b-instruct ‚Äî –±–µ—Ä—ë–º –µ—ë.
    3) –ò–Ω–∞—á–µ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –Ω–∞ HF: Qwen/Qwen2.5-7B-Instruct.
    """
    env_name = os.getenv("BASE_MODEL")
    if env_name:
        return env_name

    local_dir = BASE_DIR / "qwen2.5-7b-instruct"
    if local_dir.exists():
        return str(local_dir)

    # fallback –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç
    return "Qwen/Qwen2.5-7B-Instruct"


def _build_quant_config():
    """
    –°–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ –¥–ª—è 4-–±–∏—Ç–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏.
    –ù–∞ 24 –ì–ë VRAM —ç—Ç–æ–≥–æ –±–æ–ª–µ–µ —á–µ–º –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, –ø–ª—é—Å –æ—Å—Ç–∞—ë—Ç—Å—è –∑–∞–ø–∞—Å –ø–æ–¥ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã LoRA.
    """
    if BitsAndBytesConfig is None:
        # –ï—Å–ª–∏ bitsandbytes –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω ‚Äî –≥—Ä—É–∑–∏–º —Ñ—É–ª–ª-precision (–º–æ–∂–µ—Ç —Å—ä–µ—Å—Ç—å 18‚Äì20 –ì–ë).
        return None

    compute_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32

    return BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=compute_dtype,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
    )


def load_tokenizer_model() -> Tuple[AutoTokenizer, AutoModelForCausalLM]:
    """
    –ì–ª–∞–≤–Ω—ã–π —Ö–µ–ª–ø–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π –≤—ã–∑—ã–≤–∞—é—Ç judge_quality_llm –∏ train_lora_writer.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        tokenizer, model
    """
    model_name = _resolve_model_name()
    print(f"[qwen_loader] ‚öôÔ∏è  BASE_MODEL = {model_name}")

    quant_config = _build_quant_config()

    # --- —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä ---
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True,
    )

    # –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–∞–¥-—Ç–æ–∫–µ–Ω, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π
    if tokenizer.pad_token_id is None:
        # —É Qwen —á–∞—Å—Ç–æ eos –∏ –µ—Å—Ç—å –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç –ø–∞–¥-—Ç–æ–∫–µ–Ω–∞
        tokenizer.pad_token = tokenizer.eos_token

    tokenizer.padding_side = "left"

    # --- –º–æ–¥–µ–ª—å ---
    model_kwargs = dict(
        trust_remote_code=True,
        device_map="auto",
    )

    if quant_config is not None:
        # 4-–±–∏—Ç–Ω—ã–π —Ä–µ–∂–∏–º —á–µ—Ä–µ–∑ BitsAndBytes (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è LoRA)
        model_kwargs["quantization_config"] = quant_config
    else:
        # –±–µ–∑ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è ‚Äî –ª—É—á—à–µ —Å—Ä–∞–∑—É –≤ bfloat16/fp16, –∏–Ω–∞—á–µ –±—É–¥–µ—Ç –∂–∏—Ä–Ω—ã–π fp32
        if torch.cuda.is_available():
            model_kwargs["torch_dtype"] = torch.bfloat16

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        **model_kwargs
    )

    return tokenizer, model

if __name__ == "__main__":
    # –ü—Ä–æ—Å—Ç–æ–π —Å–∞–º–æ—Ç–µ—Å—Ç: –≥—Ä—É–∑–∏–º –º–æ–¥–µ–ª—å –∏ –≤—ã–≤–æ–¥–∏–º –∏–Ω—Ñ—É
    from datetime import datetime

    print(f"[{datetime.now().isoformat()}] üîç –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—É—Å–∫ qwen_loader")
    tokenizer, model = load_tokenizer_model()

    try:
        device = model.device
    except Exception:
        params = list(model.parameters())
        device = params[0].device if params else "cpu"

    print(f"[{datetime.now().isoformat()}] ‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    print(f"  ‚Ä¢ device: {device}")
    print(f"  ‚Ä¢ pad_token_id: {tokenizer.pad_token_id}")
    print(f"  ‚Ä¢ vocab_size: {model.config.vocab_size}")
